{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelisation Benchmark (Strong Scaling)\n",
    "\n",
    "A benchmarking workflow to measure **parallel scaling efficiency** of the OpenFOAM\n",
    "CFD solver on the target HPC cluster.\n",
    "\n",
    "The experiment fixes **one terrain**, **one wind direction**, and **one mesh** and\n",
    "submits the same simulation with a range of core counts.  The resulting wall-clock\n",
    "times are used to compute:\n",
    "\n",
    "- **Speedup** = T(baseline) / T(N)\n",
    "- **Efficiency** = Speedup / N\n",
    "\n",
    "where `T(baseline)` is the wall time at the smallest core count tested.\n",
    "\n",
    "The parallel decomposition is controlled by `system/decomposeParDict` inside each\n",
    "case directory.  All variants share the **same mesh** — only the decomposition\n",
    "and the SLURM resource allocation differ.\n",
    "\n",
    "**Node arithmetic:** the cluster has **128 cores per node**, so:\n",
    "```\n",
    "nodes             = ceil(n_cores / 128)\n",
    "ntasks_per_node   = min(n_cores, 128)\n",
    "```\n",
    "\n",
    "**Resume-safe:** Close and reopen at any time.  \n",
    "All decisions are derived from `benchmark_metadata.json` files written into each\n",
    "variant directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit these settings before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ── Paths ────────────────────────────────────────────────────────────────────\n",
    "# Root of the CFD-dataset repository (directory containing this notebook)\n",
    "REPO_ROOT = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Path to the single terrain directory to use for this benchmark\n",
    "# (e.g. a folder produced by generateInputs.py under Data/downloads/)\n",
    "FIXED_TERRAIN_DIR = os.path.join(REPO_ROOT, \"Data\", \"downloads\", \"terrain_0001_example\")\n",
    "\n",
    "# Single wind direction to test (degrees, 0 = North)\n",
    "FIXED_ROTATION_DEG = 270\n",
    "\n",
    "# terrain_config.yaml — used as-is (no mesh modifications for this benchmark)\n",
    "TERRAIN_CONFIG_PATH = os.path.join(REPO_ROOT, \"terrain_config.yaml\")\n",
    "\n",
    "# Root output directory: one sub-folder will be created per core-count variant\n",
    "CASES_OUTPUT_DIR = os.path.join(REPO_ROOT, \"parallelisation_benchmark\")\n",
    "\n",
    "# Path to the taskManager submodule\n",
    "TASK_MANAGER_DIR = os.path.join(REPO_ROOT, \"taskManager\")\n",
    "\n",
    "# Remote HPC path on Deucalion (used by taskManager for rsync/sbatch)\n",
    "DEUCALION_PATH = \"/projects/EEHPC-BEN-2026B02-011/cfd_data\"\n",
    "\n",
    "# Number of parallel workers for local meshing\n",
    "N_PARALLEL_WORKERS = 4\n",
    "\n",
    "# ── Core-count configuration ──────────────────────────────────────────────────\n",
    "# Number of physical cores per node on the cluster — do not change\n",
    "CORES_PER_NODE = 128\n",
    "\n",
    "# List of total core counts to benchmark (strong scaling series)\n",
    "CORES_TO_TEST = [16, 32, 64, 128, 256, 512]\n",
    "\n",
    "# SLURM settings common to all jobs\n",
    "SLURM_PARTITION = \"hpc\"\n",
    "SLURM_TIME      = \"04:00:00\"   # wall-clock time limit per job\n",
    "\n",
    "\n",
    "def compute_slurm_resources(n_cores: int, cores_per_node: int = CORES_PER_NODE) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Compute the SLURM --nodes and --ntasks-per-node values for a given\n",
    "    total core count, assuming a fixed number of cores per node.\n",
    "\n",
    "    The default value of *cores_per_node* is taken from the cell-level constant\n",
    "    ``CORES_PER_NODE`` (defined above in this same cell), so the function must\n",
    "    be called after that constant is set.  Pass an explicit value to override.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (nodes, ntasks_per_node)\n",
    "    \"\"\"\n",
    "    nodes            = math.ceil(n_cores / cores_per_node)\n",
    "    ntasks_per_node  = min(n_cores, cores_per_node)\n",
    "    return nodes, ntasks_per_node\n",
    "\n",
    "\n",
    "# ── Submodule path setup ──────────────────────────────────────────────────────\n",
    "for _submod in [\"terrain_following_mesh_generator\", \"ABL_BC_generator\", TASK_MANAGER_DIR]:\n",
    "    _p = _submod if os.path.isabs(_submod) else os.path.join(REPO_ROOT, _submod)\n",
    "    if _p not in sys.path:\n",
    "        sys.path.insert(0, _p)\n",
    "\n",
    "print(f\"REPO_ROOT           : {REPO_ROOT}\")\n",
    "print(f\"FIXED_TERRAIN_DIR   : {FIXED_TERRAIN_DIR}\")\n",
    "print(f\"FIXED_ROTATION_DEG  : {FIXED_ROTATION_DEG}\")\n",
    "print(f\"TERRAIN_CONFIG_PATH : {TERRAIN_CONFIG_PATH}\")\n",
    "print(f\"CASES_OUTPUT_DIR    : {CASES_OUTPUT_DIR}\")\n",
    "print(f\"CORES_PER_NODE      : {CORES_PER_NODE}\")\n",
    "print(f\"CORES_TO_TEST       : {CORES_TO_TEST}\")\n",
    "print()\n",
    "print(\"SLURM resource allocation per variant:\")\n",
    "for n in CORES_TO_TEST:\n",
    "    nodes, ntpn = compute_slurm_resources(n)\n",
    "    print(f\"  {n:>4} cores → {nodes} node(s), {ntpn} tasks/node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "# ── terrain_following_mesh_generator (submodule) ──────────────────────────────\n",
    "try:\n",
    "    from terrain_following_mesh_generator import terrain_mesh as tm\n",
    "    _MESH_OK = True\n",
    "    print(\"✓ terrain_following_mesh_generator imported\")\n",
    "except ImportError as _e:\n",
    "    _MESH_OK = False\n",
    "    print(f\"✗ terrain_following_mesh_generator not available: {_e}\")\n",
    "    print(\"  Run: git submodule update --init --recursive\")\n",
    "\n",
    "# ── taskManager (submodule) ───────────────────────────────────────────────────\n",
    "try:\n",
    "    from taskManager import OpenFOAMCaseGenerator\n",
    "    _TM_OK = True\n",
    "    print(\"✓ taskManager imported\")\n",
    "except ImportError as _e:\n",
    "    _TM_OK = False\n",
    "    OpenFOAMCaseGenerator = None\n",
    "    print(f\"✗ taskManager not available: {_e}\")\n",
    "    print(\"  Run: git submodule update --init --recursive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Parallel Cases\n",
    "\n",
    "For each core count in `CORES_TO_TEST`:\n",
    "1. Compute `nodes` and `ntasks_per_node` via `compute_slurm_resources()`.\n",
    "2. Create a case directory `parallel_bench_{n_cores}cores_{rotation:03d}deg/`.\n",
    "3. Write a `benchmark_metadata.json` recording the variant parameters.\n",
    "\n",
    "**All variants use the same terrain mesh** (no mesh configuration changes).\n",
    "The only difference between variants is the parallel decomposition and the\n",
    "SLURM resource allocation.\n",
    "\n",
    "Variants that already have a `benchmark_metadata.json` are **skipped** (resume-safe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parallel_bench_dir(cases_output_dir: str, n_cores: int, rotation: int) -> Path:\n",
    "    return Path(cases_output_dir) / f\"parallel_bench_{n_cores}cores_{rotation:03d}deg\"\n",
    "\n",
    "\n",
    "# ── Ensure output root exists ─────────────────────────────────────────────────\n",
    "Path(CASES_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ── Create variant directories and metadata ───────────────────────────────────\n",
    "for n_cores in CORES_TO_TEST:\n",
    "    nodes, ntasks_per_node = compute_slurm_resources(n_cores)\n",
    "    var_dir   = _parallel_bench_dir(CASES_OUTPUT_DIR, n_cores, FIXED_ROTATION_DEG)\n",
    "    meta_file = var_dir / \"benchmark_metadata.json\"\n",
    "\n",
    "    # Resume check — skip if metadata already written\n",
    "    if meta_file.exists():\n",
    "        print(f\"  ↷ {n_cores} cores: metadata already exists, skipping\")\n",
    "        continue\n",
    "\n",
    "    var_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    meta = {\n",
    "        \"n_cores\"         : n_cores,\n",
    "        \"nodes\"           : nodes,\n",
    "        \"ntasks_per_node\" : ntasks_per_node,\n",
    "        \"rotation\"        : FIXED_ROTATION_DEG,\n",
    "        \"terrain_dir\"     : str(FIXED_TERRAIN_DIR),\n",
    "        \"status\"          : \"pending\",\n",
    "        \"created_at\"      : datetime.now().isoformat(),\n",
    "    }\n",
    "    with open(meta_file, \"w\") as fh:\n",
    "        json.dump(meta, fh, indent=2)\n",
    "    print(f\"  ✓ {n_cores} cores: created {var_dir.name}\")\n",
    "\n",
    "print(\"\\nVariant directory setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Status Scanner\n",
    "\n",
    "Reads every `benchmark_metadata.json` found under `CASES_OUTPUT_DIR` and assembles a\n",
    "pandas DataFrame.  **Re-run this cell at any time to refresh the view.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_parallel_bench_status(cases_output_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Scan variant directories and return a status DataFrame.\"\"\"\n",
    "    output_path = Path(cases_output_dir)\n",
    "    records = []\n",
    "\n",
    "    if not output_path.exists():\n",
    "        print(f\"⚠  Output directory does not exist yet: {cases_output_dir}\")\n",
    "        print(\"   Run Section 3 first to create the variant directories.\")\n",
    "    else:\n",
    "        for meta_file in sorted(output_path.glob(\"parallel_bench_*/benchmark_metadata.json\")):\n",
    "            try:\n",
    "                with open(meta_file) as fh:\n",
    "                    meta = json.load(fh)\n",
    "            except (json.JSONDecodeError, OSError) as exc:\n",
    "                print(f\"⚠  Could not read {meta_file}: {exc}\")\n",
    "                continue\n",
    "\n",
    "            # Merge in case_status.json from taskManager if present\n",
    "            case_status_file = meta_file.parent / \"case_status.json\"\n",
    "            case_status      = {}\n",
    "            if case_status_file.exists():\n",
    "                try:\n",
    "                    with open(case_status_file) as fh:\n",
    "                        case_status = json.load(fh)\n",
    "                except (json.JSONDecodeError, OSError):\n",
    "                    pass\n",
    "\n",
    "            records.append({\n",
    "                \"n_cores\"         : meta.get(\"n_cores\"),\n",
    "                \"nodes\"           : meta.get(\"nodes\"),\n",
    "                \"ntasks_per_node\" : meta.get(\"ntasks_per_node\"),\n",
    "                \"pipeline_status\" : meta.get(\"status\"),\n",
    "                \"mesh_status\"     : case_status.get(\"mesh_status\"),\n",
    "                \"job_id\"          : case_status.get(\"job_id\"),\n",
    "                \"job_status\"      : case_status.get(\"job_status\"),\n",
    "                \"wall_time\"       : case_status.get(\"wall_time\"),\n",
    "                \"last_checked\"    : case_status.get(\"last_checked\"),\n",
    "                \"variant_dir\"     : str(meta_file.parent),\n",
    "            })\n",
    "\n",
    "    columns = [\n",
    "        \"n_cores\", \"nodes\", \"ntasks_per_node\", \"pipeline_status\",\n",
    "        \"mesh_status\", \"job_id\", \"job_status\", \"wall_time\", \"last_checked\", \"variant_dir\",\n",
    "    ]\n",
    "    df = pd.DataFrame(records, columns=columns) if records else pd.DataFrame(columns=columns)\n",
    "    # Sort by core count for readability\n",
    "    if not df.empty and \"n_cores\" in df.columns:\n",
    "        df = df.sort_values(\"n_cores\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\n",
    "print(f\"Scanned {len(df_pbench)} variant(s) at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_ICONS = {\n",
    "    \"complete\" : \"★\",\n",
    "    \"pending\"  : \"○\",\n",
    "    \"failed\"   : \"✗\",\n",
    "    \"running\"  : \"▶\",\n",
    "    \"meshed\"   : \"✓\",\n",
    "    \"meshing\"  : \"⟳\",\n",
    "}\n",
    "\n",
    "total = len(df_pbench)\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  PARALLELISATION BENCHMARK — STATUS SUMMARY\")\n",
    "print(f\"  Terrain  : {Path(FIXED_TERRAIN_DIR).name}\")\n",
    "print(f\"  Rotation : {FIXED_ROTATION_DEG}°\")\n",
    "print(f\"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  Total variants : {total}\")\n",
    "print()\n",
    "if total > 0:\n",
    "    counts = df_pbench[\"pipeline_status\"].value_counts()\n",
    "    for status, n in counts.items():\n",
    "        icon = STATUS_ICONS.get(str(status), \"?\")\n",
    "        print(f\"  {icon}  {str(status):<16} : {n}\")\n",
    "print(f\"{'='*55}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Full variant table ────────────────────────────────────────────────────────\n",
    "if not df_pbench.empty:\n",
    "    display(\n",
    "        df_pbench[\n",
    "            [\"n_cores\", \"nodes\", \"ntasks_per_node\", \"pipeline_status\",\n",
    "             \"mesh_status\", \"job_id\", \"job_status\", \"wall_time\", \"last_checked\"]\n",
    "        ].reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    print(\"No variants found. Run Section 3 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate & Mesh Cases\n",
    "\n",
    "All parallelisation benchmark variants share **the same mesh** — the mesh is\n",
    "built once (for the first variant or the shared case) and then the decomposition\n",
    "is varied in Section 7.\n",
    "\n",
    "This section uses `taskManager` to:\n",
    "1. Generate OpenFOAM case directories from the fixed terrain inputs.\n",
    "2. Run `blockMesh` / `snappyHexMesh` for cases that have not yet been meshed.\n",
    "\n",
    "**Resume-safe:** cases that already have `mesh_status == DONE` are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _TM_OK:\n",
    "    print(\"✗ taskManager not available — cannot generate or mesh cases.\")\n",
    "    print(\"  Run: git submodule update --init --recursive\")\n",
    "else:\n",
    "    generator = OpenFOAMCaseGenerator(\n",
    "        template_path=os.path.join(TASK_MANAGER_DIR, \"template\"),\n",
    "        input_dir=CASES_OUTPUT_DIR,\n",
    "        output_dir=CASES_OUTPUT_DIR,\n",
    "        deucalion_path=DEUCALION_PATH,\n",
    "    )\n",
    "\n",
    "    df_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\n",
    "\n",
    "    # ── Generate OpenFOAM cases for variants that do not have one yet ─────────\n",
    "    for _, row in df_pbench.iterrows():\n",
    "        n_cores  = row[\"n_cores\"]\n",
    "        var_dir  = Path(row[\"variant_dir\"])\n",
    "        cs_file  = var_dir / \"case_status.json\"\n",
    "\n",
    "        if cs_file.exists():\n",
    "            with open(cs_file) as fh:\n",
    "                cs = json.load(fh)\n",
    "            if (cs.get(\"mesh_status\") or \"\").upper() in (\"DONE\", \"IN_PROGRESS\"):\n",
    "                print(f\"  ↷ {n_cores} cores: already meshed / meshing, skipping case generation\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            output_path = generator.setup_case({\"variant_dir\": str(var_dir), \"n_cores\": n_cores})\n",
    "            print(f\"  ✓ {n_cores} cores: case created at {output_path}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"  ✗ {n_cores} cores: case setup failed: {exc}\")\n",
    "\n",
    "    # ── Mesh cases that are ready ──────────────────────────────────────────────\n",
    "    # Since all variants share the same mesh parameters, we mesh every case that\n",
    "    # has not yet been meshed.  In practice only the first run will do real work.\n",
    "    df_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\n",
    "    ready_cases = [\n",
    "        str(Path(row[\"variant_dir\"]))\n",
    "        for _, row in df_pbench.iterrows()\n",
    "        if row[\"mesh_status\"] in (None, \"NOT_RUN\", \"\")\n",
    "    ]\n",
    "\n",
    "    if not ready_cases:\n",
    "        print(\"\\nNo cases ready for meshing.\")\n",
    "    else:\n",
    "        print(f\"\\nMeshing {len(ready_cases)} case(s) with {N_PARALLEL_WORKERS} worker(s)…\")\n",
    "        results = generator.mesh_cases_parallel(ready_cases, n_workers=N_PARALLEL_WORKERS)\n",
    "        succeeded = sum(results)\n",
    "        failed    = len(results) - succeeded\n",
    "        print(f\"Meshing complete: {succeeded} succeeded, {failed} failed.\")\n",
    "        print(\"Re-run Section 4 to refresh the dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Update decomposeParDict\n",
    "\n",
    "Write or overwrite `system/decomposeParDict` for each variant case so that\n",
    "OpenFOAM uses the correct number of subdomains when the solver is started in\n",
    "parallel.\n",
    "\n",
    "The `scotch` method is used by default because it requires no geometric input\n",
    "from the user and typically produces well-balanced partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMPOSE_METHOD = \"scotch\"   # alternative: \"simple\" (requires coefficients)\n",
    "\n",
    "\n",
    "def write_decompose_par_dict(case_dir: Path, n_subdomains: int, method: str = \"scotch\") -> Path:\n",
    "    \"\"\"\n",
    "    Write (or overwrite) system/decomposeParDict for *case_dir*.\n",
    "\n",
    "    Returns the path of the written file.\n",
    "    \"\"\"\n",
    "    system_dir = case_dir / \"system\"\n",
    "    system_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    content = f\"\"\"/*--------------------------------*- C++ -*----------------------------------*\\\\\n",
    "  =========                 |\n",
    "  \\\\\\\\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox\n",
    "   \\\\\\\\    /   O peration     |\n",
    "    \\\\\\\\  /    A nd           |\n",
    "     \\\\\\\\/     M anipulation  |\n",
    "\\\\*---------------------------------------------------------------------------*/\n",
    "FoamFile\n",
    "{{\n",
    "    version     2.0;\n",
    "    format      ascii;\n",
    "    class       dictionary;\n",
    "    location    \"system\";\n",
    "    object      decomposeParDict;\n",
    "}}\n",
    "// * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * //\n",
    "\n",
    "numberOfSubdomains  {n_subdomains};\n",
    "\n",
    "method              {method};\n",
    "\n",
    "// ************************************************************************* //\n",
    "\"\"\"\n",
    "    out_path = system_dir / \"decomposeParDict\"\n",
    "    out_path.write_text(content)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# ── Apply to every variant ────────────────────────────────────────────────────\n",
    "df_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\n",
    "\n",
    "for _, row in df_pbench.iterrows():\n",
    "    n_cores = int(row[\"n_cores\"])\n",
    "    var_dir = Path(row[\"variant_dir\"])\n",
    "\n",
    "    out_path = write_decompose_par_dict(var_dir, n_subdomains=n_cores, method=DECOMPOSE_METHOD)\n",
    "    print(f\"  ✓ {n_cores:>4} cores → {out_path.relative_to(Path(CASES_OUTPUT_DIR))}\")\n",
    "\n",
    "# ── Show the generated file for the first variant as a sanity check ───────────\n",
    "if not df_pbench.empty:\n",
    "    sample_path = Path(df_pbench.iloc[0][\"variant_dir\"]) / \"system\" / \"decomposeParDict\"\n",
    "    if sample_path.exists():\n",
    "        print(f\"\\nSample decomposeParDict ({df_pbench.iloc[0]['n_cores']} cores):\")\n",
    "        print(sample_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Submit to HPC\n",
    "\n",
    "Submits each meshed variant to SLURM.  The `--nodes` and `--ntasks-per-node`\n",
    "SLURM options are set **per variant** according to `compute_slurm_resources()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _TM_OK:\n",
    "    print(\"✗ taskManager not available — cannot submit jobs.\")\n",
    "else:\n",
    "    df_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\n",
    "\n",
    "    # Select meshed variants that have not yet been submitted\n",
    "    ready_to_submit = df_pbench[\n",
    "        (df_pbench[\"mesh_status\"] == \"DONE\") &\n",
    "        (df_pbench[\"job_id\"].isna())\n",
    "    ]\n",
    "\n",
    "    if ready_to_submit.empty:\n",
    "        print(\"No meshed cases ready for submission.\")\n",
    "        print(\"Possible reasons:\")\n",
    "        print(\"  • Meshing has not finished yet — re-run Section 6.\")\n",
    "        print(\"  • All variants have already been submitted.\")\n",
    "    else:\n",
    "        print(f\"Submitting {len(ready_to_submit)} variant(s) to SLURM…\")\n",
    "        for _, row in ready_to_submit.iterrows():\n",
    "            n_cores         = int(row[\"n_cores\"])\n",
    "            nodes           = int(row[\"nodes\"])\n",
    "            ntasks_per_node = int(row[\"ntasks_per_node\"])\n",
    "            var_path        = row[\"variant_dir\"]\n",
    "            try:\n",
    "                job_id = generator.submit_case(\n",
    "                    case_path=var_path,\n",
    "                    partition=SLURM_PARTITION,\n",
    "                    time=SLURM_TIME,\n",
    "                    nodes=nodes,\n",
    "                    ntasks_per_node=ntasks_per_node,\n",
    "                )\n",
    "                print(f\"  ✓ {n_cores:>4} cores ({nodes}×{ntasks_per_node}): job_id={job_id}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"  ✗ {n_cores:>4} cores: submission failed: {exc}\")\n",
    "\n",
    "        print(\"\\nSubmission complete. Re-run Section 4 to refresh the dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Refresh Job Statuses\n",
    "\n",
    "Polls the SLURM scheduler for every submitted variant and writes the updated status\n",
    "back to each `case_status.json`.  Requires SSH access to the `deucalion` host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _TM_OK:\n",
    "    print(\"✗ taskManager not available.\")\n",
    "else:\n",
    "    df_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\n",
    "    submitted = df_pbench[df_pbench[\"job_id\"].notna()]\n",
    "\n",
    "    if submitted.empty:\n",
    "        print(\"No submitted jobs to refresh.\")\n",
    "    else:\n",
    "        print(f\"Refreshing {len(submitted)} job status(es)…\")\n",
    "        for _, row in submitted.iterrows():\n",
    "            try:\n",
    "                new_status = generator.update_job_status(row[\"variant_dir\"])\n",
    "                print(f\"  {int(row['n_cores']):>4} cores: {new_status}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"  ✗ {int(row['n_cores']):>4} cores: {exc}\")\n",
    "\n",
    "        print(\"\\nJob statuses updated. Re-run Section 4 to refresh the dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results: Scaling Analysis\n",
    "\n",
    "Parse wall-clock times from SLURM log files or `log.simpleFoam` and compute\n",
    "the classic strong-scaling metrics:\n",
    "\n",
    "| Metric | Formula |\n",
    "|---|---|\n",
    "| Speedup | T(N_baseline) / T(N) |\n",
    "| Efficiency | Speedup / N × 100 % |\n",
    "\n",
    "Re-run this cell after all jobs have reached `COMPLETED` status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    _PLT_OK = True\n",
    "except ImportError:\n",
    "    _PLT_OK = False\n",
    "    print(\"⚠  matplotlib not available — plots will be skipped.\")\n",
    "\n",
    "\n",
    "def _parse_solver_time(case_path: Path) -> float | None:\n",
    "    \"\"\"\n",
    "    Extract total execution time (seconds) from log.simpleFoam.\n",
    "    Looks for the final 'ExecutionTime = X s' line.\n",
    "    \"\"\"\n",
    "    log_file = case_path / \"log.simpleFoam\"\n",
    "    if not log_file.exists():\n",
    "        return None\n",
    "    text = log_file.read_text(errors=\"replace\")\n",
    "    matches = re.findall(r\"ExecutionTime\\s*=\\s*([0-9.]+)\\s*s\", text)\n",
    "    if matches:\n",
    "        try:\n",
    "            return float(matches[-1])\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# ── Build results table ───────────────────────────────────────────────────────\n",
    "df_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\n",
    "\n",
    "results = []\n",
    "for _, row in df_pbench.iterrows():\n",
    "    n_cores   = int(row[\"n_cores\"])\n",
    "    var_path  = Path(row[\"variant_dir\"])\n",
    "    # Prefer wall_time from case_status.json; fall back to solver log\n",
    "    wall_time = row.get(\"wall_time\") or _parse_solver_time(var_path)\n",
    "    results.append({\n",
    "        \"n_cores\"    : n_cores,\n",
    "        \"nodes\"      : row[\"nodes\"],\n",
    "        \"job_status\" : row[\"job_status\"],\n",
    "        \"wall_time_s\": wall_time,\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(\"n_cores\").reset_index(drop=True)\n",
    "\n",
    "# ── Compute speedup and efficiency ────────────────────────────────────────────\n",
    "t_baseline = None\n",
    "n_baseline = None\n",
    "valid_times = df_results[df_results[\"wall_time_s\"].notna()]\n",
    "if not valid_times.empty:\n",
    "    t_baseline = float(valid_times.iloc[0][\"wall_time_s\"])\n",
    "    n_baseline = int(valid_times.iloc[0][\"n_cores\"])\n",
    "\n",
    "def _speedup(t, t0):\n",
    "    return t0 / t if (t and t0 and t > 0) else None\n",
    "\n",
    "def _efficiency(speedup, n, n0):\n",
    "    return speedup / (n / n0) * 100 if (speedup and n and n0) else None\n",
    "\n",
    "df_results[\"speedup\"]    = df_results[\"wall_time_s\"].apply(lambda t: _speedup(t, t_baseline))\n",
    "df_results[\"efficiency\"] = df_results.apply(\n",
    "    lambda r: _efficiency(r[\"speedup\"], r[\"n_cores\"], n_baseline), axis=1\n",
    ")\n",
    "\n",
    "print(\"Scaling analysis:\")\n",
    "display(df_results.reset_index(drop=True))\n",
    "\n",
    "if t_baseline is None:\n",
    "    print(\"\\n⚠  No completed jobs with wall times yet.\")\n",
    "    print(\"   Wait for jobs to finish and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Scaling plots ─────────────────────────────────────────────────────────────\n",
    "if not _PLT_OK:\n",
    "    print(\"matplotlib not available — install with: pip install matplotlib\")\n",
    "elif valid_times.empty:\n",
    "    print(\"No completed results to plot yet.\")\n",
    "else:\n",
    "    plot_df = df_results.dropna(subset=[\"wall_time_s\"])\n",
    "    cores   = plot_df[\"n_cores\"].astype(int).tolist()\n",
    "    speedup = plot_df[\"speedup\"].tolist()\n",
    "    effic   = plot_df[\"efficiency\"].tolist()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # ── Speedup plot ──────────────────────────────────────────────────────────\n",
    "    axes[0].plot(cores, speedup, \"o-\", color=\"steelblue\", linewidth=2, label=\"Measured\")\n",
    "    axes[0].plot(\n",
    "        [cores[0], cores[-1]],\n",
    "        [1, cores[-1] / cores[0]],\n",
    "        \"--\", color=\"gray\", linewidth=1, label=\"Ideal (linear)\"\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Number of cores\")\n",
    "    axes[0].set_ylabel(\"Speedup\")\n",
    "    axes[0].set_title(\"Strong Scaling — Speedup\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # ── Efficiency plot ───────────────────────────────────────────────────────\n",
    "    axes[1].plot(cores, effic, \"s-\", color=\"darkorange\", linewidth=2)\n",
    "    axes[1].axhline(y=100, color=\"gray\", linestyle=\"--\", linewidth=1, label=\"Ideal (100%)\")\n",
    "    axes[1].set_xlabel(\"Number of cores\")\n",
    "    axes[1].set_ylabel(\"Parallel efficiency (%)\")\n",
    "    axes[1].set_title(\"Strong Scaling — Efficiency\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
