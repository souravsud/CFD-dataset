{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Parallelisation Benchmark (Strong Scaling)\n\nA benchmarking workflow to measure **parallel scaling efficiency** of the OpenFOAM\nCFD solver on the target HPC cluster.\n\nThe experiment fixes **one terrain**, **one wind direction**, and **one mesh** and\nsubmits the same simulation with a range of core counts.  The resulting wall-clock\ntimes are used to compute:\n\n- **Speedup** = T(baseline) / T(N)\n- **Efficiency** = Speedup / N\n\nwhere `T(baseline)` is the wall time at the smallest core count tested.\n\n## How copying works — one transfer, not N\n\nAll variants share **exactly the same mesh**.  To avoid transferring the full\ncase directory once per core count, the workflow is structured as follows:\n\n1. **Mesh once** — a single `BASE_CASE_DIR` is built locally.\n2. **Copy once** — `BASE_CASE_DIR` is rsynced to the cluster once\n   (`REMOTE_BASE_CASE_PATH`).\n3. **Per-variant job scripts** — for each core count a small SLURM batch script\n   is generated (< 1 KB) and uploaded to the cluster.  When the job starts,\n   the script does a fast **local** `cp -r` on the cluster from the shared base\n   to a variant subdirectory, writes the correct `decomposeParDict`, and then\n   runs `decomposePar` + `simpleFoam -parallel`.  No large data transfer\n   is repeated.\n\n**Node arithmetic:** the cluster has **128 cores per node**, so:\n```\nnodes             = ceil(n_cores / 128)\nntasks_per_node   = min(n_cores, 128)\n```\n\n**Resume-safe:** Close and reopen at any time.  \nAll decisions are derived from `benchmark_metadata.json` files written into each\nvariant directory."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Configuration\n\nEdit these settings before running the notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import math\nimport os\nimport sys\n\n# ── Paths ────────────────────────────────────────────────────────────────────\n# Root of the CFD-dataset repository (directory containing this notebook)\nREPO_ROOT = os.path.dirname(os.path.abspath(\"__file__\"))\n\n# Path to the single terrain directory to use for this benchmark\n# (e.g. a folder produced by generateInputs.py under Data/downloads/)\nFIXED_TERRAIN_DIR = os.path.join(REPO_ROOT, \"Data\", \"downloads\", \"terrain_0001_example\")\n\n# Single wind direction to test (degrees, 0 = North)\nFIXED_ROTATION_DEG = 270\n\n# terrain_config.yaml — used as-is (no mesh modifications for this benchmark)\nTERRAIN_CONFIG_PATH = os.path.join(REPO_ROOT, \"terrain_config.yaml\")\n\n# Root output directory for all benchmark artefacts\nCASES_OUTPUT_DIR = os.path.join(REPO_ROOT, \"parallelisation_benchmark\")\n\n# ── Single base case (shared mesh) ───────────────────────────────────────────\n# All variants share this one meshed case.  Only the decomposition and SLURM\n# resource allocation differ between variants.\nBASE_CASE_DIR = os.path.join(\n    CASES_OUTPUT_DIR, f\"base_case_{FIXED_ROTATION_DEG:03d}deg\"\n)\n\n# ── HPC paths ─────────────────────────────────────────────────────────────────\n# Hostname alias used for SSH / rsync (must be configured in ~/.ssh/config)\nHPC_HOST = \"deucalion\"\n\n# Remote root directory on the cluster\nDEUCALION_PATH = \"/projects/EEHPC-BEN-2026B02-011/cfd_data\"\n\n# Remote path where the single base case is copied (done ONCE)\nREMOTE_BASE_CASE_PATH = (\n    f\"{DEUCALION_PATH}/parallelisation_benchmark/base_case_{FIXED_ROTATION_DEG:03d}deg\"\n)\n\n# Remote root for per-variant directories created by the job scripts on the cluster\nREMOTE_VARIANTS_ROOT = f\"{DEUCALION_PATH}/parallelisation_benchmark\"\n\n# ── taskManager submodule ─────────────────────────────────────────────────────\nTASK_MANAGER_DIR = os.path.join(REPO_ROOT, \"taskManager\")\n\n# Number of parallel workers for local meshing\nN_PARALLEL_WORKERS = 4\n\n# ── Core-count configuration ──────────────────────────────────────────────────\n# Number of physical cores per node on the cluster — do not change\nCORES_PER_NODE = 128\n\n# List of total core counts to benchmark (strong scaling series)\nCORES_TO_TEST = [16, 32, 64, 128, 256, 512]\n\n# SLURM settings common to all jobs\nSLURM_PARTITION = \"hpc\"\nSLURM_TIME      = \"04:00:00\"   # wall-clock time limit per job\n\n\ndef compute_slurm_resources(n_cores: int, cores_per_node: int = CORES_PER_NODE) -> tuple[int, int]:\n    \"\"\"\n    Compute the SLURM --nodes and --ntasks-per-node values for a given\n    total core count, assuming a fixed number of cores per node.\n\n    Returns\n    -------\n    (nodes, ntasks_per_node)\n    \"\"\"\n    nodes            = math.ceil(n_cores / cores_per_node)\n    ntasks_per_node  = min(n_cores, cores_per_node)\n    return nodes, ntasks_per_node\n\n\n# ── Submodule path setup ──────────────────────────────────────────────────────\nfor _submod in [\"terrain_following_mesh_generator\", \"ABL_BC_generator\", TASK_MANAGER_DIR]:\n    _p = _submod if os.path.isabs(_submod) else os.path.join(REPO_ROOT, _submod)\n    if _p not in sys.path:\n        sys.path.insert(0, _p)\n\nprint(f\"REPO_ROOT              : {REPO_ROOT}\")\nprint(f\"FIXED_TERRAIN_DIR      : {FIXED_TERRAIN_DIR}\")\nprint(f\"FIXED_ROTATION_DEG     : {FIXED_ROTATION_DEG}\")\nprint(f\"TERRAIN_CONFIG_PATH    : {TERRAIN_CONFIG_PATH}\")\nprint(f\"CASES_OUTPUT_DIR       : {CASES_OUTPUT_DIR}\")\nprint(f\"BASE_CASE_DIR          : {BASE_CASE_DIR}\")\nprint(f\"REMOTE_BASE_CASE_PATH  : {REMOTE_BASE_CASE_PATH}\")\nprint(f\"REMOTE_VARIANTS_ROOT   : {REMOTE_VARIANTS_ROOT}\")\nprint(f\"CORES_PER_NODE         : {CORES_PER_NODE}\")\nprint(f\"CORES_TO_TEST          : {CORES_TO_TEST}\")\nprint()\nprint(\"SLURM resource allocation per variant:\")\nfor n in CORES_TO_TEST:\n    nodes, ntpn = compute_slurm_resources(n)\n    print(f\"  {n:>4} cores → {nodes} node(s), {ntpn} tasks/node\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport subprocess\nfrom pathlib import Path\nfrom datetime import datetime\n\nimport yaml\nimport pandas as pd\n\n# ── terrain_following_mesh_generator (submodule) ──────────────────────────────\ntry:\n    from terrain_following_mesh_generator import terrain_mesh as tm\n    _MESH_OK = True\n    print(\"✓ terrain_following_mesh_generator imported\")\nexcept ImportError as _e:\n    _MESH_OK = False\n    print(f\"✗ terrain_following_mesh_generator not available: {_e}\")\n    print(\"  Run: git submodule update --init --recursive\")\n\n# ── taskManager (submodule) ───────────────────────────────────────────────────\ntry:\n    from taskManager import OpenFOAMCaseGenerator\n    _TM_OK = True\n    print(\"✓ taskManager imported\")\nexcept ImportError as _e:\n    _TM_OK = False\n    OpenFOAMCaseGenerator = None\n    print(f\"✗ taskManager not available: {_e}\")\n    print(\"  Run: git submodule update --init --recursive\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Create Variant Metadata\n\nFor each core count in `CORES_TO_TEST`:\n1. Compute `nodes` and `ntasks_per_node` via `compute_slurm_resources()`.\n2. Create a lightweight variant directory `parallel_bench_{n_cores}cores_{rotation:03d}deg/`.\n3. Write `benchmark_metadata.json` recording the variant parameters.\n\n**No mesh data is duplicated here.** The mesh lives exclusively in `BASE_CASE_DIR`.\nThese directories later receive `system/decomposeParDict` and a SLURM run script\n(both tiny), which are the only files uploaded to the cluster per variant.\n\nVariants that already have a `benchmark_metadata.json` are **skipped** (resume-safe)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _parallel_bench_dir(cases_output_dir: str, n_cores: int, rotation: int) -> Path:\n    return Path(cases_output_dir) / f\"parallel_bench_{n_cores}cores_{rotation:03d}deg\"\n\n\n# ── Ensure output root exists ─────────────────────────────────────────────────\nPath(CASES_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n\n# ── Create variant directories and metadata ───────────────────────────────────\nfor n_cores in CORES_TO_TEST:\n    nodes, ntasks_per_node = compute_slurm_resources(n_cores)\n    var_dir   = _parallel_bench_dir(CASES_OUTPUT_DIR, n_cores, FIXED_ROTATION_DEG)\n    meta_file = var_dir / \"benchmark_metadata.json\"\n\n    # Resume check — skip if metadata already written\n    if meta_file.exists():\n        print(f\"  ↷ {n_cores} cores: metadata already exists, skipping\")\n        continue\n\n    var_dir.mkdir(parents=True, exist_ok=True)\n\n    remote_variant_path = (\n        f\"{REMOTE_VARIANTS_ROOT}/parallel_bench_{n_cores}cores_{FIXED_ROTATION_DEG:03d}deg\"\n    )\n    meta = {\n        \"n_cores\"             : n_cores,\n        \"nodes\"               : nodes,\n        \"ntasks_per_node\"     : ntasks_per_node,\n        \"rotation\"            : FIXED_ROTATION_DEG,\n        \"terrain_dir\"         : str(FIXED_TERRAIN_DIR),\n        \"base_case_dir\"       : str(BASE_CASE_DIR),\n        \"remote_base_case\"    : REMOTE_BASE_CASE_PATH,\n        \"remote_variant_path\" : remote_variant_path,\n        \"status\"              : \"pending\",\n        \"created_at\"          : datetime.now().isoformat(),\n    }\n    with open(meta_file, \"w\") as fh:\n        json.dump(meta, fh, indent=2)\n    print(f\"  ✓ {n_cores} cores: created {var_dir.name}\")\n\nprint(\"\\nVariant directory setup complete.\")\nprint(f\"Shared base case (to be meshed once): {BASE_CASE_DIR}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Status Scanner\n\nReads every `benchmark_metadata.json` found under `CASES_OUTPUT_DIR` and assembles a\npandas DataFrame.  **Re-run this cell at any time to refresh the view.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def scan_parallel_bench_status(cases_output_dir: str) -> pd.DataFrame:\n    \"\"\"Scan variant directories and return a status DataFrame.\"\"\"\n    output_path = Path(cases_output_dir)\n    records = []\n\n    if not output_path.exists():\n        print(f\"⚠  Output directory does not exist yet: {cases_output_dir}\")\n        print(\"   Run Section 3 first to create the variant directories.\")\n    else:\n        for meta_file in sorted(output_path.glob(\"parallel_bench_*/benchmark_metadata.json\")):\n            try:\n                with open(meta_file) as fh:\n                    meta = json.load(fh)\n            except (json.JSONDecodeError, OSError) as exc:\n                print(f\"⚠  Could not read {meta_file}: {exc}\")\n                continue\n\n            # Read per-variant submission status (written by Section 8)\n            submit_status_file = meta_file.parent / \"submit_status.json\"\n            submit_status      = {}\n            if submit_status_file.exists():\n                try:\n                    with open(submit_status_file) as fh:\n                        submit_status = json.load(fh)\n                except (json.JSONDecodeError, OSError):\n                    pass\n\n            records.append({\n                \"n_cores\"             : meta.get(\"n_cores\"),\n                \"nodes\"               : meta.get(\"nodes\"),\n                \"ntasks_per_node\"     : meta.get(\"ntasks_per_node\"),\n                \"pipeline_status\"     : meta.get(\"status\"),\n                \"base_copied_to_hpc\"  : meta.get(\"base_copied_to_hpc\", False),\n                \"job_id\"              : submit_status.get(\"job_id\"),\n                \"job_status\"          : submit_status.get(\"job_status\"),\n                \"wall_time\"           : submit_status.get(\"wall_time\"),\n                \"last_checked\"        : submit_status.get(\"last_checked\"),\n                \"variant_dir\"         : str(meta_file.parent),\n                \"remote_variant_path\" : meta.get(\"remote_variant_path\"),\n            })\n\n    # Also check base case mesh status\n    base_meta_file = Path(BASE_CASE_DIR) / \"benchmark_metadata.json\"\n    base_mesh_status = \"not_built\"\n    if base_meta_file.exists():\n        try:\n            bm = json.load(open(base_meta_file))\n            base_mesh_status = bm.get(\"mesh_status\", \"unknown\")\n        except Exception:\n            pass\n    print(f\"Base case mesh status : {base_mesh_status}  ({BASE_CASE_DIR})\")\n\n    columns = [\n        \"n_cores\", \"nodes\", \"ntasks_per_node\", \"pipeline_status\",\n        \"base_copied_to_hpc\", \"job_id\", \"job_status\", \"wall_time\",\n        \"last_checked\", \"variant_dir\", \"remote_variant_path\",\n    ]\n    df = pd.DataFrame(records, columns=columns) if records else pd.DataFrame(columns=columns)\n    if not df.empty and \"n_cores\" in df.columns:\n        df = df.sort_values(\"n_cores\").reset_index(drop=True)\n    return df\n\n\ndf_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\nprint(f\"Scanned {len(df_pbench)} variant(s) at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Summary Dashboard"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "STATUS_ICONS = {\n    \"complete\" : \"★\",\n    \"pending\"  : \"○\",\n    \"failed\"   : \"✗\",\n    \"running\"  : \"▶\",\n    \"meshed\"   : \"✓\",\n    \"meshing\"  : \"⟳\",\n}\n\ntotal = len(df_pbench)\nprint(f\"{'='*55}\")\nprint(f\"  PARALLELISATION BENCHMARK — STATUS SUMMARY\")\nprint(f\"  Terrain  : {Path(FIXED_TERRAIN_DIR).name}\")\nprint(f\"  Rotation : {FIXED_ROTATION_DEG}°\")\nprint(f\"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"{'='*55}\")\nprint(f\"  Total variants : {total}\")\nprint()\nif total > 0:\n    counts = df_pbench[\"pipeline_status\"].value_counts()\n    for status, n in counts.items():\n        icon = STATUS_ICONS.get(str(status), \"?\")\n        print(f\"  {icon}  {str(status):<16} : {n}\")\nprint(f\"{'='*55}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Full variant table ────────────────────────────────────────────────────────\nif not df_pbench.empty:\n    display(\n        df_pbench[\n            [\"n_cores\", \"nodes\", \"ntasks_per_node\", \"pipeline_status\",\n             \"base_copied_to_hpc\", \"job_id\", \"job_status\", \"wall_time\", \"last_checked\"]\n        ].reset_index(drop=True)\n    )\nelse:\n    print(\"No variants found. Run Section 3 first.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Build the Shared Base Case (mesh once)\n\nGenerates and meshes **one** OpenFOAM case from the fixed terrain inputs.\nThis is the only case that is ever meshed — all core-count variants reuse it.\n\nThe base case is placed in `BASE_CASE_DIR` (configured in Section 1).\nIf it already has `mesh_status == DONE` the cell is a no-op (resume-safe).\n\n> **Why only one case?**  \n> The parallel decomposition in OpenFOAM is done at *runtime* by `decomposePar`;\n> the underlying mesh is identical regardless of how many cores are used.  \n> Meshing N copies would waste both local disk and HPC transfer time."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not _TM_OK:\n    print(\"✗ taskManager not available — cannot generate or mesh the base case.\")\n    print(\"  Run: git submodule update --init --recursive\")\nelse:\n    generator = OpenFOAMCaseGenerator(\n        template_path=os.path.join(TASK_MANAGER_DIR, \"template\"),\n        input_dir=str(FIXED_TERRAIN_DIR),\n        output_dir=CASES_OUTPUT_DIR,\n        deucalion_path=DEUCALION_PATH,\n    )\n\n    # ── Check whether the base case has already been meshed ───────────────────\n    base_meta   = Path(BASE_CASE_DIR) / \"benchmark_metadata.json\"\n    already_done = False\n    if base_meta.exists():\n        try:\n            bm = json.load(open(base_meta))\n            already_done = (bm.get(\"mesh_status\", \"\").upper() == \"DONE\")\n        except Exception:\n            pass\n\n    if already_done:\n        print(f\"↷ Base case already meshed — skipping.  ({BASE_CASE_DIR})\")\n    else:\n        # ── Generate the case (creates directory + OpenFOAM structure) ──────────\n        Path(BASE_CASE_DIR).mkdir(parents=True, exist_ok=True)\n        print(f\"Generating base case at {BASE_CASE_DIR} …\")\n        try:\n            case_info = {\n                \"terrain_dir\"      : str(FIXED_TERRAIN_DIR),\n                \"rotation_degree\"  : FIXED_ROTATION_DEG,\n                \"output_dir\"       : str(BASE_CASE_DIR),\n            }\n            output_path = generator.setup_case(case_info)\n            print(f\"  ✓ Base case created at {output_path}\")\n        except Exception as exc:\n            print(f\"  ✗ Case setup failed: {exc}\")\n\n        # ── Mesh the base case ────────────────────────────────────────────────\n        print(f\"Meshing base case with {N_PARALLEL_WORKERS} worker(s) …\")\n        results = generator.mesh_cases_parallel([str(BASE_CASE_DIR)], n_workers=N_PARALLEL_WORKERS)\n        if results and results[0]:\n            print(\"  ✓ Meshing succeeded.\")\n            # Record mesh_status in base case metadata\n            bm = {\"mesh_status\": \"DONE\", \"meshed_at\": datetime.now().isoformat()}\n            with open(base_meta, \"w\") as fh:\n                json.dump(bm, fh, indent=2)\n        else:\n            print(\"  ✗ Meshing failed — check logs.\")\n\n    print(\"\\nRe-run Section 4 to refresh the dashboard.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Write Per-Variant `decomposeParDict`\n\nWrite (or overwrite) `system/decomposeParDict` in each lightweight variant directory.\nThe `scotch` method is used by default — it requires no geometric input and\ntypically produces well-balanced partitions.\n\nThese tiny files are later uploaded to the cluster alongside the per-variant\nSLURM run script.  **The base case mesh data is never re-uploaded.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DECOMPOSE_METHOD = \"scotch\"   # alternative: \"simple\" (requires coefficients)\n\n\ndef write_decompose_par_dict(case_dir: Path, n_subdomains: int, method: str = \"scotch\") -> Path:\n    \"\"\"\n    Write (or overwrite) system/decomposeParDict for *case_dir*.\n    Returns the path of the written file.\n    \"\"\"\n    system_dir = case_dir / \"system\"\n    system_dir.mkdir(parents=True, exist_ok=True)\n\n    content = f\"\"\"/*--------------------------------*- C++ -*----------------------------------*\\\\\\\n\\n  =========                 |\n  \\\\\\\\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox\n   \\\\\\\\    /   O peration     |\n    \\\\\\\\  /    A nd           |\n     \\\\\\\\/     M anipulation  |\n\\\\*---------------------------------------------------------------------------*/\nFoamFile\n{{\n    version     2.0;\n    format      ascii;\n    class       dictionary;\n    location    \\\"system\\\";\n    object      decomposeParDict;\n}}\n// * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * //\n\nnumberOfSubdomains  {n_subdomains};\n\nmethod              {method};\n\n// ************************************************************************* //\n\"\"\"\n    out_path = system_dir / \"decomposeParDict\"\n    out_path.write_text(content)\n    return out_path\n\n\n# ── Apply to every variant ────────────────────────────────────────────────────\ndf_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\n\nfor _, row in df_pbench.iterrows():\n    n_cores = int(row[\"n_cores\"])\n    var_dir = Path(row[\"variant_dir\"])\n\n    out_path = write_decompose_par_dict(var_dir, n_subdomains=n_cores, method=DECOMPOSE_METHOD)\n    print(f\"  ✓ {n_cores:>4} cores → {out_path.relative_to(Path(CASES_OUTPUT_DIR))}\")\n\n# ── Show the generated file for the first variant as a sanity check ───────────\nif not df_pbench.empty:\n    sample_path = Path(df_pbench.iloc[0][\"variant_dir\"]) / \"system\" / \"decomposeParDict\"\n    if sample_path.exists():\n        print(f\"\\nSample decomposeParDict ({df_pbench.iloc[0]['n_cores']} cores):\")\n        print(sample_path.read_text())\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Copy Base Case to HPC (one-time transfer)\n\nRsyncs the single meshed `BASE_CASE_DIR` to `REMOTE_BASE_CASE_PATH` on the cluster.\n\nThis transfer happens **once**.  Subsequent variants do **not** re-upload the mesh\ndata — their SLURM job scripts perform a fast `cp -r` from the remote base\ndirectory to the variant directory entirely within the cluster filesystem.\n\nThe cell is **resume-safe**: if `base_copied_to_hpc` is already `true` in the\nvariant metadata files the rsync is skipped.  Set `FORCE_RECOPY = True` to\noverride and re-transfer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "FORCE_RECOPY = False   # set True to force re-transfer of the base case\n\n# ── Check whether the base case has already been copied ──────────────────────\n_base_already_copied = False\ndf_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\nif not df_pbench.empty:\n    _base_already_copied = bool(df_pbench[\"base_copied_to_hpc\"].iloc[0])\n\nif _base_already_copied and not FORCE_RECOPY:\n    print(\"↷ Base case already flagged as copied to HPC — skipping rsync.\")\n    print(\"  Set FORCE_RECOPY = True to re-transfer.\")\nelse:\n    base_src = str(BASE_CASE_DIR).rstrip(\"/\") + \"/\"\n    remote_dest = f\"{HPC_HOST}:{REMOTE_BASE_CASE_PATH}/\"\n\n    print(f\"Copying base case to HPC (one-time transfer)\")\n    print(f\"  Local  : {base_src}\")\n    print(f\"  Remote : {remote_dest}\")\n    print()\n\n    rsync_cmd = [\n        \"rsync\", \"-az\", \"--progress\",\n        \"--exclude=processor*/\",   # exclude any pre-existing decomposed data\n        base_src,\n        remote_dest,\n    ]\n    try:\n        result = subprocess.run(rsync_cmd, check=True, capture_output=False, text=True)\n        print(\"  ✓ Base case transferred successfully.\")\n\n        # ── Mark all variants as having the base copied ───────────────────────\n        for meta_file in sorted(Path(CASES_OUTPUT_DIR).glob(\n                \"parallel_bench_*/benchmark_metadata.json\")):\n            try:\n                with open(meta_file) as fh:\n                    meta = json.load(fh)\n                meta[\"base_copied_to_hpc\"] = True\n                meta[\"base_copied_at\"]     = datetime.now().isoformat()\n                with open(meta_file, \"w\") as fh:\n                    json.dump(meta, fh, indent=2)\n            except Exception as exc:\n                print(f\"  ⚠  Could not update metadata for {meta_file.parent.name}: {exc}\")\n\n        print(\"  Variant metadata updated (base_copied_to_hpc = true).\")\n    except subprocess.CalledProcessError as exc:\n        print(f\"  ✗ rsync failed (exit code {exc.returncode}).\")\n        print(\"  Check that the 'deucalion' host is reachable and ~/.ssh/config is set up.\")\n    except FileNotFoundError:\n        print(\"  ✗ rsync not found on PATH.\")\n        print(\"  Install rsync or manually copy BASE_CASE_DIR to the cluster before submitting.\")\n\nprint(\"\\nRe-run Section 4 to refresh the dashboard.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Submit Per-Variant Jobs to HPC\n\nFor each core-count variant this cell:\n1. Generates a **self-contained SLURM batch script** locally (< 1 KB).\n2. Uploads that script **and** the matching `decomposeParDict` to the cluster\n   (two tiny files, no mesh data).\n3. Submits the script via `sbatch`.\n\nWhen the job starts on the cluster the script:\n- Does a fast **local** `cp -r <remote_base> <remote_variant>` inside the\n  cluster filesystem (no network transfer).\n- Overwrites `system/decomposeParDict` with the variant-specific file.\n- Runs `decomposePar` followed by `mpirun simpleFoam -parallel`.\n\n**Pre-requisites:** The base case must have been copied (Section 8).\nVariants that already have a `job_id` in `submit_status.json` are skipped\n(resume-safe)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _generate_slurm_script(\n    n_cores: int,\n    nodes: int,\n    ntasks_per_node: int,\n    remote_base: str,\n    remote_variant: str,\n    decompose_dict_content: str,\n    partition: str,\n    time_limit: str,\n    job_name: str,\n) -> str:\n    \"\"\"\n    Return a SLURM batch script string.\n\n    The script copies the pre-uploaded base case to a variant directory on the\n    cluster (fast local cp, no network transfer), installs the correct\n    decomposeParDict, decomposes, and runs the solver in parallel.\n    \"\"\"\n    # Escape the decomposeParDict content for the heredoc\n    dict_escaped = decompose_dict_content.replace(\"'\", \"'\\\"'\\\"'\")\n    return f\"\"\"#!/bin/bash\n#SBATCH --job-name={job_name}\n#SBATCH --nodes={nodes}\n#SBATCH --ntasks-per-node={ntasks_per_node}\n#SBATCH --partition={partition}\n#SBATCH --time={time_limit}\n#SBATCH --output={remote_variant}/slurm_%j.out\n#SBATCH --error={remote_variant}/slurm_%j.err\n\nset -euo pipefail\n\nBASE_DIR=\\\"{remote_base}\\\"\nVARIANT_DIR=\\\"{remote_variant}\\\"\n\n# ── Fast local copy of the shared base case (no network transfer) ──────────\nif [ ! -d \\\"$VARIANT_DIR/constant\\\" ]; then\n    echo \\\"Copying base case to variant directory…\\\"\n    cp -r \\\"$BASE_DIR/.\\\" \\\"$VARIANT_DIR/\\\"\nfi\n\n# ── Install the correct decomposeParDict ────────────────────────────────────\nmkdir -p \\\"$VARIANT_DIR/system\\\"\ncat > \\\"$VARIANT_DIR/system/decomposeParDict\\\" << 'DICTEOF'\n{dict_escaped}\nDICTEOF\n\n# ── Decompose and run ───────────────────────────────────────────────────────\ncd \\\"$VARIANT_DIR\\\"\ndecomposePar -force > log.decomposePar 2>&1\nmpirun -np {n_cores} simpleFoam -parallel > log.simpleFoam 2>&1\n\"\"\"\n\n\ndf_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\n\n# Only submit variants whose base case has been copied and that have no job yet\nready = df_pbench[\n    (df_pbench[\"base_copied_to_hpc\"] == True) &\n    (df_pbench[\"job_id\"].isna())\n]\n\nif ready.empty:\n    print(\"No variants ready for submission.\")\n    print(\"Possible reasons:\")\n    print(\"  • Base case has not been copied yet — run Section 8.\")\n    print(\"  • All variants have already been submitted.\")\nelse:\n    print(f\"Submitting {len(ready)} variant(s) to SLURM…\")\n\n    for _, row in ready.iterrows():\n        n_cores         = int(row[\"n_cores\"])\n        nodes           = int(row[\"nodes\"])\n        ntasks_per_node = int(row[\"ntasks_per_node\"])\n        var_dir         = Path(row[\"variant_dir\"])\n        remote_variant  = row[\"remote_variant_path\"]\n        job_name        = f\"pbench_{n_cores}c_{FIXED_ROTATION_DEG}deg\"\n\n        # ── Read the decomposeParDict written in Section 7 ────────────────────\n        decomp_file = var_dir / \"system\" / \"decomposeParDict\"\n        if not decomp_file.exists():\n            print(f\"  ✗ {n_cores} cores: decomposeParDict not found — run Section 7 first.\")\n            continue\n        decompose_dict_content = decomp_file.read_text()\n\n        # ── Generate SLURM script ─────────────────────────────────────────────\n        slurm_script = _generate_slurm_script(\n            n_cores=n_cores,\n            nodes=nodes,\n            ntasks_per_node=ntasks_per_node,\n            remote_base=REMOTE_BASE_CASE_PATH,\n            remote_variant=remote_variant,\n            decompose_dict_content=decompose_dict_content,\n            partition=SLURM_PARTITION,\n            time_limit=SLURM_TIME,\n            job_name=job_name,\n        )\n\n        # ── Write SLURM script locally ─────────────────────────────────────────\n        local_script = var_dir / \"run.sh\"\n        local_script.write_text(slurm_script)\n\n        # ── Upload decomposeParDict + run script (two tiny files) ─────────────\n        remote_system_dir = f\"{remote_variant}/system\"\n        try:\n            # Ensure remote variant + system directories exist\n            subprocess.run(\n                [\"ssh\", HPC_HOST, f\"mkdir -p {remote_system_dir}\"],\n                check=True, capture_output=True, text=True,\n            )\n            # Upload decomposeParDict\n            subprocess.run(\n                [\"rsync\", \"-az\", str(decomp_file),\n                 f\"{HPC_HOST}:{remote_system_dir}/decomposeParDict\"],\n                check=True, capture_output=True, text=True,\n            )\n            # Upload run script\n            subprocess.run(\n                [\"rsync\", \"-az\", str(local_script),\n                 f\"{HPC_HOST}:{remote_variant}/run.sh\"],\n                check=True, capture_output=True, text=True,\n            )\n\n            # ── sbatch ────────────────────────────────────────────────────────\n            sbatch_result = subprocess.run(\n                [\"ssh\", HPC_HOST, f\"sbatch {remote_variant}/run.sh\"],\n                check=True, capture_output=True, text=True,\n            )\n            # Parse job ID from \"Submitted batch job XXXXXX\"\n            job_id = None\n            for tok in sbatch_result.stdout.split():\n                if tok.isdigit():\n                    job_id = tok\n                    break\n\n            # ── Persist submission status ──────────────────────────────────────\n            submit_status = {\n                \"job_id\"       : job_id,\n                \"job_status\"   : \"PENDING\",\n                \"submitted_at\" : datetime.now().isoformat(),\n                \"wall_time\"    : None,\n                \"last_checked\" : datetime.now().isoformat(),\n            }\n            with open(var_dir / \"submit_status.json\", \"w\") as fh:\n                json.dump(submit_status, fh, indent=2)\n\n            print(f\"  ✓ {n_cores:>4} cores ({nodes}×{ntasks_per_node}): \"\n                  f\"job_id={job_id}  (uploaded: decomposeParDict + run.sh only)\")\n\n        except subprocess.CalledProcessError as exc:\n            err = exc.stderr.strip() if exc.stderr else str(exc)\n            print(f\"  ✗ {n_cores:>4} cores: submission failed: {err}\")\n        except FileNotFoundError as exc:\n            print(f\"  ✗ {n_cores:>4} cores: command not found ({exc})\")\n            print(\"    Make sure rsync and ssh are available and the host alias is configured.\")\n            break  # no point continuing if rsync/ssh are missing\n\n    print(\"\\nSubmission complete. Re-run Section 4 to refresh the dashboard.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Refresh Job Statuses\n\nPolls SLURM (`sacct`) on the cluster for every submitted variant and writes the\nupdated status (including elapsed wall time) back to `submit_status.json`.\nRequires SSH access to the `deucalion` host."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\nsubmitted = df_pbench[df_pbench[\"job_id\"].notna()]\n\nif submitted.empty:\n    print(\"No submitted jobs to refresh.\")\nelse:\n    print(f\"Refreshing {len(submitted)} job status(es) via sacct…\")\n    for _, row in submitted.iterrows():\n        job_id  = str(row[\"job_id\"])\n        n_cores = int(row[\"n_cores\"])\n        var_dir = Path(row[\"variant_dir\"])\n\n        try:\n            result = subprocess.run(\n                [\n                    \"ssh\", HPC_HOST,\n                    f\"sacct -j {job_id} --format=State,Elapsed --noheader --parsable2\",\n                ],\n                capture_output=True, text=True, check=True,\n            )\n            lines = [l for l in result.stdout.strip().splitlines() if l]\n            job_status = wall_time = None\n            if lines:\n                parts      = lines[0].split(\"|\")\n                job_status = parts[0].strip() if parts else None\n                wall_time  = parts[1].strip() if len(parts) > 1 else None\n\n            submit_status_file = var_dir / \"submit_status.json\"\n            try:\n                with open(submit_status_file) as fh:\n                    ss = json.load(fh)\n            except Exception:\n                ss = {}\n            ss[\"job_status\"]   = job_status\n            ss[\"wall_time\"]    = wall_time\n            ss[\"last_checked\"] = datetime.now().isoformat()\n            with open(submit_status_file, \"w\") as fh:\n                json.dump(ss, fh, indent=2)\n\n            icon = {\"COMPLETED\": \"★\", \"RUNNING\": \"▶\", \"PENDING\": \"○\",\n                    \"FAILED\": \"✗\", \"CANCELLED\": \"✗\"}.get(str(job_status), \"?\")\n            print(f\"  {icon} {n_cores:>4} cores (job {job_id}): {job_status}  elapsed={wall_time}\")\n\n        except subprocess.CalledProcessError as exc:\n            print(f\"  ✗ {n_cores:>4} cores: sacct query failed: {exc.stderr.strip()}\")\n        except FileNotFoundError:\n            print(\"  ✗ ssh not found — cannot poll SLURM.\")\n            break\n\n    print(\"\\nJob statuses updated. Re-run Section 4 to refresh the dashboard.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Results: Scaling Analysis\n\nParse wall-clock times from `submit_status.json` or `log.simpleFoam` (fetched\nfrom the cluster) and compute the classic strong-scaling metrics:\n\n| Metric | Formula |\n|---|---|\n| Speedup | T(N_baseline) / T(N) |\n| Efficiency | Speedup / N × 100 % |\n\nRe-run this cell after all jobs have reached `COMPLETED` status."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\n\ntry:\n    import matplotlib.pyplot as plt\n    _PLT_OK = True\nexcept ImportError:\n    _PLT_OK = False\n    print(\"⚠  matplotlib not available — plots will be skipped.\")\n\n\ndef _parse_solver_time_remote(n_cores: int, remote_variant: str) -> float | None:\n    \"\"\"\n    Fetch the final ExecutionTime from log.simpleFoam on the cluster via SSH.\n    Returns elapsed seconds or None if unavailable.\n    \"\"\"\n    try:\n        result = subprocess.run(\n            [\"ssh\", HPC_HOST,\n             f\"grep 'ExecutionTime' {remote_variant}/log.simpleFoam 2>/dev/null | tail -1\"],\n            capture_output=True, text=True, check=False,\n        )\n        m = re.search(r\"ExecutionTime\\s*=\\s*([0-9.]+)\\s*s\", result.stdout)\n        return float(m.group(1)) if m else None\n    except Exception:\n        return None\n\n\n# ── Build results table ───────────────────────────────────────────────────────\ndf_pbench = scan_parallel_bench_status(CASES_OUTPUT_DIR)\n\nresults = []\nfor _, row in df_pbench.iterrows():\n    n_cores        = int(row[\"n_cores\"])\n    remote_variant = row.get(\"remote_variant_path\") or \"\"\n    # Prefer wall_time from submit_status.json (updated by Section 10)\n    wall_time_str  = row.get(\"wall_time\")\n    wall_time_s    = None\n    if wall_time_str and isinstance(wall_time_str, str) and \":\" in wall_time_str:\n        # sacct format HH:MM:SS or MM:SS\n        parts = wall_time_str.split(\":\")\n        try:\n            if len(parts) == 3:\n                wall_time_s = int(parts[0])*3600 + int(parts[1])*60 + float(parts[2])\n            elif len(parts) == 2:\n                wall_time_s = int(parts[0])*60 + float(parts[1])\n        except ValueError:\n            pass\n    if wall_time_s is None and remote_variant:\n        # Fall back to parsing log.simpleFoam on the cluster\n        wall_time_s = _parse_solver_time_remote(n_cores, remote_variant)\n    results.append({\n        \"n_cores\"    : n_cores,\n        \"nodes\"      : row[\"nodes\"],\n        \"job_status\" : row[\"job_status\"],\n        \"wall_time_s\": wall_time_s,\n    })\n\ndf_results = pd.DataFrame(results).sort_values(\"n_cores\").reset_index(drop=True)\n\n# ── Compute speedup and efficiency ────────────────────────────────────────────\nvalid_times = df_results[df_results[\"wall_time_s\"].notna()]\nt_baseline  = float(valid_times.iloc[0][\"wall_time_s\"]) if not valid_times.empty else None\nn_baseline  = int(valid_times.iloc[0][\"n_cores\"])       if not valid_times.empty else None\n\ndef _speedup(t, t0):\n    return t0 / t if (t and t0 and t > 0) else None\n\ndef _efficiency(speedup, n, n0):\n    return speedup / (n / n0) * 100 if (speedup and n and n0) else None\n\ndf_results[\"speedup\"]    = df_results[\"wall_time_s\"].apply(lambda t: _speedup(t, t_baseline))\ndf_results[\"efficiency\"] = df_results.apply(\n    lambda r: _efficiency(r[\"speedup\"], r[\"n_cores\"], n_baseline), axis=1\n)\n\nprint(\"Scaling analysis:\")\ndisplay(df_results.reset_index(drop=True))\n\nif t_baseline is None:\n    print(\"\\n⚠  No completed jobs with wall times yet.\")\n    print(\"   Wait for jobs to finish and re-run this cell.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Scaling plots ─────────────────────────────────────────────────────────────\nif not _PLT_OK:\n    print(\"matplotlib not available — install with: pip install matplotlib\")\nelif valid_times.empty:\n    print(\"No completed results to plot yet.\")\nelse:\n    plot_df = df_results.dropna(subset=[\"wall_time_s\"])\n    cores   = plot_df[\"n_cores\"].astype(int).tolist()\n    speedup = plot_df[\"speedup\"].tolist()\n    effic   = plot_df[\"efficiency\"].tolist()\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    # ── Speedup plot ──────────────────────────────────────────────────────────\n    axes[0].plot(cores, speedup, \"o-\", color=\"steelblue\", linewidth=2, label=\"Measured\")\n    axes[0].plot(\n        [cores[0], cores[-1]],\n        [1, cores[-1] / cores[0]],\n        \"--\", color=\"gray\", linewidth=1, label=\"Ideal (linear)\"\n    )\n    axes[0].set_xlabel(\"Number of cores\")\n    axes[0].set_ylabel(\"Speedup\")\n    axes[0].set_title(\"Strong Scaling — Speedup\")\n    axes[0].legend()\n    axes[0].grid(True, linestyle=\"--\", alpha=0.5)\n\n    # ── Efficiency plot ───────────────────────────────────────────────────────\n    axes[1].plot(cores, effic, \"s-\", color=\"darkorange\", linewidth=2)\n    axes[1].axhline(y=100, color=\"gray\", linestyle=\"--\", linewidth=1, label=\"Ideal (100%)\")\n    axes[1].set_xlabel(\"Number of cores\")\n    axes[1].set_ylabel(\"Parallel efficiency (%)\")\n    axes[1].set_title(\"Strong Scaling — Efficiency\")\n    axes[1].legend()\n    axes[1].grid(True, linestyle=\"--\", alpha=0.5)\n\n    plt.tight_layout()\n    plt.show()\n"
  }
 ]
}