{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Independence Test\n",
    "\n",
    "A benchmarking workflow to verify that CFD results are **mesh-independent**.\n",
    "\n",
    "The experiment fixes **one terrain** and **one wind direction** and runs the same\n",
    "simulation with a family of progressively finer mesh configurations. If the\n",
    "flow statistics (e.g. velocity profiles, turbulence quantities) converge as the\n",
    "mesh is refined, the coarsest mesh that achieves the desired accuracy level\n",
    "can be selected for the full dataset generation run.\n",
    "\n",
    "**Workflow overview:**\n",
    "1. For each mesh variant, copy the fixed terrain inputs and write a modified\n",
    "   `terrain_config.yaml` with the variant-specific grid / mesh overrides.\n",
    "2. Run the terrain mesh pipeline (or skip if already done) to produce OpenFOAM\n",
    "   case inputs — one per variant.\n",
    "3. Generate OpenFOAM cases and mesh them locally (or on HPC).\n",
    "4. Submit all variants to SLURM and monitor their progress.\n",
    "5. Compare key metrics across variants to establish mesh independence.\n",
    "\n",
    "**Resume-safe:** Close and reopen at any time.  \n",
    "All decisions are derived from `benchmark_metadata.json` files written into each\n",
    "variant directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit these settings before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# ── Paths ────────────────────────────────────────────────────────────────────\n",
    "# Root of the CFD-dataset repository (directory containing this notebook)\n",
    "REPO_ROOT = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Path to the single terrain directory to use for this benchmark\n",
    "# (e.g. a folder produced by generateInputs.py under Data/downloads/)\n",
    "FIXED_TERRAIN_DIR = os.path.join(REPO_ROOT, \"Data\", \"downloads\", \"terrain_0001_example\")\n",
    "\n",
    "# Single wind direction to test (degrees, 0 = North)\n",
    "FIXED_ROTATION_DEG = 270\n",
    "\n",
    "# Base terrain configuration file — variant overrides are layered on top of this\n",
    "TERRAIN_CONFIG_PATH = os.path.join(REPO_ROOT, \"terrain_config.yaml\")\n",
    "\n",
    "# Root output directory: one sub-folder will be created per mesh variant\n",
    "CASES_OUTPUT_DIR = os.path.join(REPO_ROOT, \"grid_independence_benchmark\")\n",
    "\n",
    "# Path to the taskManager submodule\n",
    "TASK_MANAGER_DIR = os.path.join(REPO_ROOT, \"taskManager\")\n",
    "\n",
    "# Remote HPC path on Deucalion (used by taskManager for rsync/sbatch)\n",
    "DEUCALION_PATH = \"/projects/EEHPC-BEN-2026B02-011/cfd_data\"\n",
    "\n",
    "# Number of parallel workers for local meshing\n",
    "N_PARALLEL_WORKERS = 4\n",
    "\n",
    "# ── Mesh variants ─────────────────────────────────────────────────────────────\n",
    "# Each entry must have a unique \"name\" key.  All other keys are deep-merged\n",
    "# into the base terrain_config.yaml before running the mesh pipeline.\n",
    "# Use the same top-level section names as terrain_config.yaml\n",
    "# (\"grid\", \"mesh\", \"terrain\", \"boundary\", …).\n",
    "MESH_VARIANTS = [\n",
    "    {\n",
    "        \"name\": \"coarse\",\n",
    "        \"grid\": {\"nx\": 181, \"ny\": 181},\n",
    "        \"mesh\": {\"total_z_cells\": 44},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"medium\",\n",
    "        \"grid\": {\"nx\": 271, \"ny\": 271},\n",
    "        \"mesh\": {\"total_z_cells\": 55},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"fine\",  # matches the baseline in terrain_config.yaml\n",
    "        \"grid\": {\"nx\": 361, \"ny\": 361},\n",
    "        \"mesh\": {\"total_z_cells\": 66},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"very_fine\",\n",
    "        \"grid\": {\"nx\": 451, \"ny\": 451},\n",
    "        \"mesh\": {\"total_z_cells\": 80},\n",
    "    },\n",
    "]\n",
    "\n",
    "# ── SLURM settings ────────────────────────────────────────────────────────────\n",
    "# These are applied to every submitted job (all variants use the same resources)\n",
    "SLURM_PARTITION        = \"hpc\"\n",
    "SLURM_TIME             = \"04:00:00\"   # wall-clock time limit per job\n",
    "SLURM_N_NODES          = 2\n",
    "SLURM_NTASKS_PER_NODE  = 128\n",
    "\n",
    "# ── Submodule path setup ──────────────────────────────────────────────────────\n",
    "for _submod in [\"terrain_following_mesh_generator\", \"ABL_BC_generator\", TASK_MANAGER_DIR]:\n",
    "    _p = _submod if os.path.isabs(_submod) else os.path.join(REPO_ROOT, _submod)\n",
    "    if _p not in sys.path:\n",
    "        sys.path.insert(0, _p)\n",
    "\n",
    "print(f\"REPO_ROOT           : {REPO_ROOT}\")\n",
    "print(f\"FIXED_TERRAIN_DIR   : {FIXED_TERRAIN_DIR}\")\n",
    "print(f\"FIXED_ROTATION_DEG  : {FIXED_ROTATION_DEG}\")\n",
    "print(f\"TERRAIN_CONFIG_PATH : {TERRAIN_CONFIG_PATH}\")\n",
    "print(f\"CASES_OUTPUT_DIR    : {CASES_OUTPUT_DIR}\")\n",
    "print(f\"N_PARALLEL_WORKERS  : {N_PARALLEL_WORKERS}\")\n",
    "print(f\"Mesh variants       : {[v['name'] for v in MESH_VARIANTS]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "# ── terrain_following_mesh_generator (submodule) ──────────────────────────────\n",
    "try:\n",
    "    from terrain_following_mesh_generator import terrain_mesh as tm\n",
    "    _MESH_OK = True\n",
    "    print(\"✓ terrain_following_mesh_generator imported\")\n",
    "except ImportError as _e:\n",
    "    _MESH_OK = False\n",
    "    print(f\"✗ terrain_following_mesh_generator not available: {_e}\")\n",
    "    print(\"  Run: git submodule update --init --recursive\")\n",
    "\n",
    "# ── taskManager (submodule) ───────────────────────────────────────────────────\n",
    "try:\n",
    "    from taskManager import OpenFOAMCaseGenerator\n",
    "    _TM_OK = True\n",
    "    print(\"✓ taskManager imported\")\n",
    "except ImportError as _e:\n",
    "    _TM_OK = False\n",
    "    OpenFOAMCaseGenerator = None\n",
    "    print(f\"✗ taskManager not available: {_e}\")\n",
    "    print(\"  Run: git submodule update --init --recursive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Mesh Variant Inputs\n",
    "\n",
    "For each entry in `MESH_VARIANTS`:\n",
    "1. Create a variant output directory (`grid_bench_{name}_{rotation:03d}deg/`).\n",
    "2. Load `terrain_config.yaml` and deep-merge the variant overrides.\n",
    "3. Save the modified config to a `variant_terrain_config.yaml` inside the variant dir.\n",
    "4. Run the terrain mesh pipeline with the modified config (if the submodule is available).\n",
    "5. Write a `benchmark_metadata.json` to record variant parameters and pipeline status.\n",
    "\n",
    "Variants that already have a `benchmark_metadata.json` with `status == \"complete\"`\n",
    "are **skipped** (resume-safe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _deep_merge(base: dict, overrides: dict) -> dict:\n",
    "    \"\"\"Recursively merge *overrides* into a deep copy of *base*.\"\"\"\n",
    "    result = copy.deepcopy(base)\n",
    "    for key, value in overrides.items():\n",
    "        if key == \"name\":\n",
    "            continue  # skip the variant name key\n",
    "        if isinstance(value, dict) and isinstance(result.get(key), dict):\n",
    "            result[key] = _deep_merge(result[key], value)\n",
    "        else:\n",
    "            result[key] = copy.deepcopy(value)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _variant_dir(cases_output_dir: str, name: str, rotation: int) -> Path:\n",
    "    return Path(cases_output_dir) / f\"grid_bench_{name}_{rotation:03d}deg\"\n",
    "\n",
    "\n",
    "def _mesh_params_summary(variant: dict) -> str:\n",
    "    \"\"\"One-liner summary of the mesh parameters for display in tables.\"\"\"\n",
    "    parts = []\n",
    "    if \"grid\" in variant:\n",
    "        g = variant[\"grid\"]\n",
    "        if \"nx\" in g and \"ny\" in g:\n",
    "            parts.append(f\"nx={g['nx']}, ny={g['ny']}\")\n",
    "    if \"mesh\" in variant:\n",
    "        m = variant[\"mesh\"]\n",
    "        if \"total_z_cells\" in m:\n",
    "            parts.append(f\"nz={m['total_z_cells']}\")\n",
    "    return \"; \".join(parts) if parts else str(variant)\n",
    "\n",
    "\n",
    "# ── Load base config ──────────────────────────────────────────────────────────\n",
    "with open(TERRAIN_CONFIG_PATH) as fh:\n",
    "    base_config = yaml.safe_load(fh)\n",
    "print(f\"✓ Loaded base config: {TERRAIN_CONFIG_PATH}\")\n",
    "\n",
    "# ── Locate terrain inputs (DEM / roughness) ───────────────────────────────────\n",
    "terrain_path = Path(FIXED_TERRAIN_DIR)\n",
    "if not terrain_path.exists():\n",
    "    print(f\"✗ FIXED_TERRAIN_DIR not found: {FIXED_TERRAIN_DIR}\")\n",
    "    print(\"  Set FIXED_TERRAIN_DIR to an existing terrain directory and re-run.\")\n",
    "else:\n",
    "    dem_files  = sorted([f for f in terrain_path.glob(\"terrain_*.tif\") if \"_raw\" not in f.name])\n",
    "    rmap_files = sorted([f for f in terrain_path.glob(\"roughness_*.tif\") if \"_raw\" not in f.name])\n",
    "    dem_file       = str(dem_files[0])  if dem_files  else None\n",
    "    roughness_file = str(rmap_files[0]) if rmap_files else None\n",
    "    print(f\"✓ Terrain dir    : {terrain_path.name}\")\n",
    "    print(f\"  DEM file       : {Path(dem_file).name if dem_file else 'NOT FOUND'}\")\n",
    "    print(f\"  Roughness file : {Path(roughness_file).name if roughness_file else 'not found'}\")\n",
    "\n",
    "# ── Generate variant inputs ───────────────────────────────────────────────────\n",
    "Path(CASES_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for variant in MESH_VARIANTS:\n",
    "    name       = variant[\"name\"]\n",
    "    var_dir    = _variant_dir(CASES_OUTPUT_DIR, name, FIXED_ROTATION_DEG)\n",
    "    meta_file  = var_dir / \"benchmark_metadata.json\"\n",
    "\n",
    "    # ── Resume check ─────────────────────────────────────────────────────────\n",
    "    if meta_file.exists():\n",
    "        with open(meta_file) as fh:\n",
    "            meta = json.load(fh)\n",
    "        if meta.get(\"status\") == \"complete\":\n",
    "            print(f\"  ↷ {name}: already complete, skipping\")\n",
    "            continue\n",
    "\n",
    "    var_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ── Build modified config ─────────────────────────────────────────────────\n",
    "    variant_config = _deep_merge(base_config, variant)\n",
    "    variant_config.setdefault(\"terrain\", {})\n",
    "    variant_config[\"terrain\"][\"rotation_deg\"] = FIXED_ROTATION_DEG\n",
    "\n",
    "    variant_config_path = var_dir / \"variant_terrain_config.yaml\"\n",
    "    with open(variant_config_path, \"w\") as fh:\n",
    "        yaml.dump(variant_config, fh, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "    # ── Initial metadata ──────────────────────────────────────────────────────\n",
    "    meta = {\n",
    "        \"name\"        : name,\n",
    "        \"mesh_params\" : _mesh_params_summary(variant),\n",
    "        \"terrain_dir\" : str(FIXED_TERRAIN_DIR),\n",
    "        \"rotation\"    : FIXED_ROTATION_DEG,\n",
    "        \"status\"      : \"pending\",\n",
    "        \"created_at\"  : datetime.now().isoformat(),\n",
    "    }\n",
    "    with open(meta_file, \"w\") as fh:\n",
    "        json.dump(meta, fh, indent=2)\n",
    "\n",
    "    # ── Run mesh pipeline ─────────────────────────────────────────────────────\n",
    "    if not _MESH_OK:\n",
    "        print(f\"  ○ {name}: mesh pipeline not available (submodule missing)\")\n",
    "        continue\n",
    "\n",
    "    if not terrain_path.exists() or dem_file is None:\n",
    "        print(f\"  ✗ {name}: FIXED_TERRAIN_DIR missing or has no DEM — skipping mesh step\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  ▶ {name}: running mesh pipeline…\")\n",
    "    try:\n",
    "        mesh_config = tm.load_config(str(variant_config_path))\n",
    "        pipeline    = tm.TerrainMeshPipeline()\n",
    "        pipeline.run(\n",
    "            dem_path=dem_file,\n",
    "            rmap_path=roughness_file,\n",
    "            output_dir=str(var_dir),\n",
    "            **mesh_config,\n",
    "        )\n",
    "        meta[\"status\"] = \"complete\"\n",
    "        print(f\"  ✓ {name}: mesh pipeline complete\")\n",
    "    except Exception as exc:\n",
    "        meta[\"status\"] = \"failed\"\n",
    "        meta[\"error\"]  = str(exc)\n",
    "        print(f\"  ✗ {name}: {exc}\")\n",
    "\n",
    "    meta[\"updated_at\"] = datetime.now().isoformat()\n",
    "    with open(meta_file, \"w\") as fh:\n",
    "        json.dump(meta, fh, indent=2)\n",
    "\n",
    "print(\"\\nVariant input generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Status Scanner\n",
    "\n",
    "Reads every `benchmark_metadata.json` found under `CASES_OUTPUT_DIR` and assembles a\n",
    "pandas DataFrame.  **Re-run this cell at any time to refresh the view.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_benchmark_status(cases_output_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Scan variant directories and return a status DataFrame.\"\"\"\n",
    "    output_path = Path(cases_output_dir)\n",
    "    records = []\n",
    "\n",
    "    if not output_path.exists():\n",
    "        print(f\"⚠  Output directory does not exist yet: {cases_output_dir}\")\n",
    "        print(\"   Run Section 3 first to generate variant inputs.\")\n",
    "    else:\n",
    "        for meta_file in sorted(output_path.glob(\"grid_bench_*/benchmark_metadata.json\")):\n",
    "            try:\n",
    "                with open(meta_file) as fh:\n",
    "                    meta = json.load(fh)\n",
    "            except (json.JSONDecodeError, OSError) as exc:\n",
    "                print(f\"⚠  Could not read {meta_file}: {exc}\")\n",
    "                continue\n",
    "\n",
    "            # Check for a sibling case_status.json written by taskManager\n",
    "            case_status_file = meta_file.parent / \"case_status.json\"\n",
    "            case_status      = {}\n",
    "            if case_status_file.exists():\n",
    "                try:\n",
    "                    with open(case_status_file) as fh:\n",
    "                        case_status = json.load(fh)\n",
    "                except (json.JSONDecodeError, OSError):\n",
    "                    pass\n",
    "\n",
    "            records.append({\n",
    "                \"variant_name\"    : meta.get(\"name\"),\n",
    "                \"mesh_params\"     : meta.get(\"mesh_params\"),\n",
    "                \"pipeline_status\" : meta.get(\"status\"),\n",
    "                \"mesh_status\"     : case_status.get(\"mesh_status\"),\n",
    "                \"job_id\"          : case_status.get(\"job_id\"),\n",
    "                \"job_status\"      : case_status.get(\"job_status\"),\n",
    "                \"last_checked\"    : case_status.get(\"last_checked\"),\n",
    "                \"variant_dir\"     : str(meta_file.parent),\n",
    "            })\n",
    "\n",
    "    columns = [\n",
    "        \"variant_name\", \"mesh_params\", \"pipeline_status\",\n",
    "        \"mesh_status\", \"job_id\", \"job_status\", \"last_checked\", \"variant_dir\",\n",
    "    ]\n",
    "    return pd.DataFrame(records, columns=columns) if records else pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "df_bench = scan_benchmark_status(CASES_OUTPUT_DIR)\n",
    "print(f\"Scanned {len(df_bench)} variant(s) at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_ICONS = {\n",
    "    \"complete\"      : \"★\",\n",
    "    \"pending\"       : \"○\",\n",
    "    \"failed\"        : \"✗\",\n",
    "    \"ready_to_mesh\" : \"○\",\n",
    "    \"meshing\"       : \"⟳\",\n",
    "    \"meshed\"        : \"✓\",\n",
    "    \"running\"       : \"▶\",\n",
    "}\n",
    "\n",
    "total = len(df_bench)\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  GRID INDEPENDENCE TEST — STATUS SUMMARY\")\n",
    "print(f\"  Terrain  : {Path(FIXED_TERRAIN_DIR).name}\")\n",
    "print(f\"  Rotation : {FIXED_ROTATION_DEG}°\")\n",
    "print(f\"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  Total variants : {total}\")\n",
    "print()\n",
    "if total > 0:\n",
    "    counts = df_bench[\"pipeline_status\"].value_counts()\n",
    "    for status, n in counts.items():\n",
    "        icon = STATUS_ICONS.get(str(status), \"?\")\n",
    "        print(f\"  {icon}  {str(status):<16} : {n}\")\n",
    "print(f\"{'='*55}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Full variant table ────────────────────────────────────────────────────────\n",
    "if not df_bench.empty:\n",
    "    display(\n",
    "        df_bench[\n",
    "            [\"variant_name\", \"mesh_params\", \"pipeline_status\",\n",
    "             \"mesh_status\", \"job_id\", \"job_status\", \"last_checked\"]\n",
    "        ].reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    print(\"No variants found. Run Section 3 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate & Mesh Cases\n",
    "\n",
    "Uses `taskManager` to convert each variant input directory into a full OpenFOAM case\n",
    "and then mesh it locally using `blockMesh` + `snappyHexMesh`.\n",
    "\n",
    "**Resume-safe:** variants that already have `mesh_status == DONE` in their\n",
    "`case_status.json` are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _TM_OK:\n",
    "    print(\"✗ taskManager not available — cannot generate or mesh cases.\")\n",
    "    print(\"  Run: git submodule update --init --recursive\")\n",
    "else:\n",
    "    # ── Initialise the case generator ─────────────────────────────────────────\n",
    "    generator = OpenFOAMCaseGenerator(\n",
    "        template_path=os.path.join(TASK_MANAGER_DIR, \"template\"),\n",
    "        input_dir=CASES_OUTPUT_DIR,\n",
    "        output_dir=CASES_OUTPUT_DIR,\n",
    "        deucalion_path=DEUCALION_PATH,\n",
    "    )\n",
    "\n",
    "    # Re-scan to pick up the latest state\n",
    "    df_bench = scan_benchmark_status(CASES_OUTPUT_DIR)\n",
    "\n",
    "    # ── Generate cases for variants that have completed mesh pipeline input ────\n",
    "    for _, row in df_bench.iterrows():\n",
    "        name      = row[\"variant_name\"]\n",
    "        var_dir   = Path(row[\"variant_dir\"])\n",
    "        cs_file   = var_dir / \"case_status.json\"\n",
    "\n",
    "        if cs_file.exists():\n",
    "            with open(cs_file) as fh:\n",
    "                cs = json.load(fh)\n",
    "            if (cs.get(\"mesh_status\") or \"\").upper() in (\"DONE\", \"IN_PROGRESS\"):\n",
    "                print(f\"  ↷ {name}: already meshed / meshing, skipping\")\n",
    "                continue\n",
    "\n",
    "        if row[\"pipeline_status\"] != \"complete\":\n",
    "            print(f\"  ○ {name}: pipeline_status={row['pipeline_status']}, skipping case generation\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            output_path = generator.setup_case({\"variant_dir\": str(var_dir), \"name\": name})\n",
    "            print(f\"  ✓ {name}: case created at {output_path}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"  ✗ {name}: case setup failed: {exc}\")\n",
    "\n",
    "    # ── Mesh cases that are ready ──────────────────────────────────────────────\n",
    "    ready_cases = [\n",
    "        str(Path(row[\"variant_dir\"]))\n",
    "        for _, row in df_bench.iterrows()\n",
    "        if row[\"mesh_status\"] in (None, \"NOT_RUN\", \"\")\n",
    "        and row[\"pipeline_status\"] == \"complete\"\n",
    "    ]\n",
    "\n",
    "    if not ready_cases:\n",
    "        print(\"\\nNo cases ready for meshing.\")\n",
    "    else:\n",
    "        print(f\"\\nMeshing {len(ready_cases)} case(s) with {N_PARALLEL_WORKERS} worker(s)…\")\n",
    "        results = generator.mesh_cases_parallel(ready_cases, n_workers=N_PARALLEL_WORKERS)\n",
    "        succeeded = sum(results)\n",
    "        failed    = len(results) - succeeded\n",
    "        print(f\"Meshing complete: {succeeded} succeeded, {failed} failed.\")\n",
    "        print(\"Re-run Section 4 to refresh the dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Submit to HPC\n",
    "\n",
    "Submits all meshed (but not yet submitted) variants to SLURM via the taskManager.\n",
    "All variants use the same SLURM resource settings defined in Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _TM_OK:\n",
    "    print(\"✗ taskManager not available — cannot submit jobs.\")\n",
    "else:\n",
    "    df_bench = scan_benchmark_status(CASES_OUTPUT_DIR)\n",
    "\n",
    "    slurm_config = {\n",
    "        \"partition\"       : SLURM_PARTITION,\n",
    "        \"time\"            : SLURM_TIME,\n",
    "        \"nodes\"           : SLURM_N_NODES,\n",
    "        \"ntasks_per_node\" : SLURM_NTASKS_PER_NODE,\n",
    "    }\n",
    "\n",
    "    meshed_cases = df_bench[\n",
    "        (df_bench[\"mesh_status\"] == \"DONE\") &\n",
    "        (df_bench[\"job_id\"].isna())\n",
    "    ]\n",
    "\n",
    "    if meshed_cases.empty:\n",
    "        print(\"No meshed cases ready for submission.\")\n",
    "        print(\"Possible reasons:\")\n",
    "        print(\"  • Meshing has not finished yet — re-run Section 6.\")\n",
    "        print(\"  • All variants have already been submitted.\")\n",
    "    else:\n",
    "        print(f\"Submitting {len(meshed_cases)} variant(s) to SLURM…\")\n",
    "        for _, row in meshed_cases.iterrows():\n",
    "            name     = row[\"variant_name\"]\n",
    "            var_path = row[\"variant_dir\"]\n",
    "            try:\n",
    "                job_id = generator.submit_case(\n",
    "                    case_path=var_path,\n",
    "                    **slurm_config,\n",
    "                )\n",
    "                print(f\"  ✓ {name}: submitted (job_id={job_id})\")\n",
    "            except Exception as exc:\n",
    "                print(f\"  ✗ {name}: submission failed: {exc}\")\n",
    "\n",
    "        print(\"\\nSubmission complete. Re-run Section 4 to refresh the dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Refresh Job Statuses\n",
    "\n",
    "Polls the SLURM scheduler for every submitted variant and writes the updated status\n",
    "back to each `case_status.json`.  Requires SSH access to the `deucalion` host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _TM_OK:\n",
    "    print(\"✗ taskManager not available.\")\n",
    "else:\n",
    "    df_bench = scan_benchmark_status(CASES_OUTPUT_DIR)\n",
    "    submitted = df_bench[df_bench[\"job_id\"].notna()]\n",
    "\n",
    "    if submitted.empty:\n",
    "        print(\"No submitted jobs to refresh.\")\n",
    "    else:\n",
    "        print(f\"Refreshing {len(submitted)} job status(es)…\")\n",
    "        for _, row in submitted.iterrows():\n",
    "            try:\n",
    "                new_status = generator.update_job_status(row[\"variant_dir\"])\n",
    "                print(f\"  {row['variant_name']}: {new_status}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"  ✗ {row['variant_name']}: {exc}\")\n",
    "\n",
    "        print(\"\\nJob statuses updated. Re-run Section 4 to refresh the dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Summary\n",
    "\n",
    "Once all variants have completed, compare the key metrics across mesh resolutions\n",
    "to assess grid independence:\n",
    "\n",
    "| Metric | Source |\n",
    "|---|---|\n",
    "| Total cell count | `constant/polyMesh/owner` or log files |\n",
    "| Wall-clock time | SLURM job accounting (`sacct`) |\n",
    "| Convergence indicator | Final residual from `log.simpleFoam` |\n",
    "\n",
    "Re-run this cell after all jobs have reached `COMPLETED` status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _parse_cell_count(case_path: Path) -> int | None:\n",
    "    \"\"\"Try to read total cell count from blockMesh or checkMesh log.\"\"\"\n",
    "    for log_name in (\"log.blockMesh\", \"log.snappyHexMesh\", \"log.checkMesh\"):\n",
    "        log_file = case_path / log_name\n",
    "        if not log_file.exists():\n",
    "            continue\n",
    "        text = log_file.read_text(errors=\"replace\")\n",
    "        # Look for \"cells:\" line\n",
    "        m = re.search(r\"cells:\\s+(\\d+)\", text)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "def _parse_final_residual(case_path: Path) -> float | None:\n",
    "    \"\"\"Extract the final Ux residual from log.simpleFoam (last occurrence).\"\"\"\n",
    "    log_file = case_path / \"log.simpleFoam\"\n",
    "    if not log_file.exists():\n",
    "        return None\n",
    "    text = log_file.read_text(errors=\"replace\")\n",
    "    # Pattern: \"Solving for Ux, Initial residual = X, Final residual = Y\"\n",
    "    matches = re.findall(r\"Solving for Ux.*?Final residual = ([0-9eE.+-]+)\", text)\n",
    "    if matches:\n",
    "        try:\n",
    "            return float(matches[-1])\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# ── Build results table ───────────────────────────────────────────────────────\n",
    "df_bench = scan_benchmark_status(CASES_OUTPUT_DIR)\n",
    "\n",
    "results = []\n",
    "for _, row in df_bench.iterrows():\n",
    "    var_path    = Path(row[\"variant_dir\"])\n",
    "    cell_count  = _parse_cell_count(var_path)\n",
    "    final_resid = _parse_final_residual(var_path)\n",
    "    results.append({\n",
    "        \"variant_name\"   : row[\"variant_name\"],\n",
    "        \"mesh_params\"    : row[\"mesh_params\"],\n",
    "        \"job_status\"     : row[\"job_status\"],\n",
    "        \"total_cells\"    : cell_count,\n",
    "        \"final_residual\" : final_resid,\n",
    "        \"wall_time\"      : None,   # populate from sacct if available\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "if df_results.empty:\n",
    "    print(\"No results yet. Wait for all jobs to complete and re-run this cell.\")\n",
    "else:\n",
    "    print(\"Grid independence results:\")\n",
    "    display(df_results.reset_index(drop=True))\n",
    "\n",
    "    completed = df_results[df_results[\"job_status\"] == \"COMPLETED\"]\n",
    "    if len(completed) < 2:\n",
    "        print(\"\\n⚠  Fewer than 2 variants completed — cannot yet assess independence.\")\n",
    "    else:\n",
    "        # Simple check: relative change in final residual between successive refinements\n",
    "        print(\"\\nRelative change in final residual between consecutive refinement levels:\")\n",
    "        resids = completed[\"final_residual\"].dropna().values\n",
    "        for i in range(1, len(resids)):\n",
    "            rel_change = abs(resids[i] - resids[i - 1]) / (abs(resids[i - 1]) + 1e-30)\n",
    "            icon = \"✓\" if rel_change < 0.05 else \"✗\"\n",
    "            print(f\"  {icon}  levels {i-1}→{i}: {rel_change*100:.2f}% change\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
