{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFD Pipeline Dashboard\n",
    "\n",
    "A lightweight workflow dashboard and controlled batch runner for the CFD pipeline.\n",
    "\n",
    "- **Dataset preparation** is handled by `generateInputs.py` (this repo).\n",
    "- **Simulation execution** is handled by `taskManager/taskManager.py` (submodule), which creates cases, meshes them, submits them to SLURM on HPC, and maintains a `case_status.json` per case.\n",
    "\n",
    "This notebook does **not** reimplement any execution logic — it only orchestrates and monitors by reading `case_status.json` files and calling existing `taskManager` methods when needed.\n",
    "\n",
    "**Resume-safe:** Close and reopen at any time. All decisions are derived from the `case_status.json` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit these paths and settings before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# ── Paths ────────────────────────────────────────────────────────────────────\n",
    "# Root of the CFD-dataset repository (directory containing this notebook)\n",
    "REPO_ROOT = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Directory that contains the generated OpenFOAM case folders\n",
    "# (each sub-folder has a case_status.json produced by taskManager)\n",
    "CASES_OUTPUT_DIR = os.path.join(REPO_ROOT, \"openFoamCases\")\n",
    "\n",
    "# OpenFOAM case template folder (used by taskManager to create new cases)\n",
    "TEMPLATE_PATH = os.path.join(REPO_ROOT, \"taskManager\", \"template\")\n",
    "\n",
    "# Input data directory (downloads from generateInputs.py)\n",
    "INPUT_DATA_DIR = os.path.join(REPO_ROOT, \"Data\", \"downloads\")\n",
    "\n",
    "# Remote HPC path on Deucalion (used by taskManager for rsync/sbatch)\n",
    "DEUCALION_PATH = \"/projects/EEHPC-BEN-2026B02-011/cfd_data\"\n",
    "\n",
    "# ── Batch settings ────────────────────────────────────────────────────────────\n",
    "# Number of cases to mesh per batch run (Cell 5)\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# Number of parallel workers for meshing\n",
    "N_PARALLEL_WORKERS = 4\n",
    "\n",
    "# ── taskManager import ────────────────────────────────────────────────────────\n",
    "# Add the taskManager submodule directory to sys.path so we can import it\n",
    "TASK_MANAGER_DIR = os.path.join(REPO_ROOT, \"taskManager\")\n",
    "if TASK_MANAGER_DIR not in sys.path:\n",
    "    sys.path.insert(0, TASK_MANAGER_DIR)\n",
    "\n",
    "print(f\"REPO_ROOT          : {REPO_ROOT}\")\n",
    "print(f\"CASES_OUTPUT_DIR   : {CASES_OUTPUT_DIR}\")\n",
    "print(f\"TEMPLATE_PATH      : {TEMPLATE_PATH}\")\n",
    "print(f\"INPUT_DATA_DIR     : {INPUT_DATA_DIR}\")\n",
    "print(f\"BATCH_SIZE         : {BATCH_SIZE}\")\n",
    "print(f\"N_PARALLEL_WORKERS : {N_PARALLEL_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from taskManager import OpenFOAMCaseGenerator\n",
    "    print(\"✓ taskManager imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Could not import taskManager: {e}\")\n",
    "    print(\"  Make sure the taskManager submodule is initialised:\")\n",
    "    print(\"    git submodule update --init --recursive\")\n",
    "    OpenFOAMCaseGenerator = None\n",
    "\n",
    "# Initialise the case generator (used later for meshing / submission)\n",
    "if OpenFOAMCaseGenerator is not None:\n",
    "    generator = OpenFOAMCaseGenerator(\n",
    "        template_path=TEMPLATE_PATH,\n",
    "        input_dir=INPUT_DATA_DIR,\n",
    "        output_dir=CASES_OUTPUT_DIR,\n",
    "        deucalion_path=DEUCALION_PATH,\n",
    "    )\n",
    "    print(\"✓ OpenFOAMCaseGenerator initialised\")\n",
    "else:\n",
    "    generator = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Status Scanner\n",
    "\n",
    "Reads every `case_status.json` found under `CASES_OUTPUT_DIR` and assembles a pandas DataFrame.  \n",
    "Each row represents one case.  **Re-run this cell at any time to refresh the view.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_pipeline_status(status: dict) -> str:\n",
    "    \"\"\"\n",
    "    Map the raw fields in case_status.json to a single human-readable\n",
    "    pipeline status string:\n",
    "\n",
    "        ready_to_mesh  – mesh has not been run yet (mesh_status == NOT_RUN)\n",
    "        meshing        – meshing is currently in progress (mesh_status == IN_PROGRESS)\n",
    "        meshed         – mesh done, not yet submitted to HPC\n",
    "        running        – submitted to HPC, job is PENDING or RUNNING\n",
    "        complete       – HPC job completed successfully\n",
    "        failed         – meshing failed, or HPC job failed/cancelled/timed-out\n",
    "        unknown        – status file present but unrecognised combination\n",
    "    \"\"\"\n",
    "    mesh_status = (status.get(\"mesh_status\") or \"NOT_RUN\").upper()\n",
    "    job_status  = (status.get(\"job_status\")  or \"\").upper()\n",
    "    submitted   = status.get(\"submitted\", False)\n",
    "\n",
    "    if mesh_status in (\"FAILED\", \"ERROR\"):\n",
    "        return \"failed\"\n",
    "    if job_status in (\"FAILED\", \"CANCELLED\", \"TIMEOUT\"):\n",
    "        return \"failed\"\n",
    "    if job_status == \"COMPLETED\":\n",
    "        return \"complete\"\n",
    "    if submitted and job_status in (\"PENDING\", \"RUNNING\", \"\"):\n",
    "        return \"running\"\n",
    "    if mesh_status == \"DONE\":\n",
    "        return \"meshed\"\n",
    "    if mesh_status == \"IN_PROGRESS\":\n",
    "        return \"meshing\"\n",
    "    if mesh_status == \"NOT_RUN\":\n",
    "        return \"ready_to_mesh\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def scan_cases(output_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Walk *output_dir* and collect every case_status.json into a DataFrame.\n",
    "    Returns an empty DataFrame (with the expected columns) if no cases exist yet.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    records = []\n",
    "\n",
    "    if not output_path.exists():\n",
    "        print(f\"⚠  Cases output directory does not exist yet: {output_dir}\")\n",
    "        print(\"   Run generateInputs.py and the taskManager case generator first.\")\n",
    "    else:\n",
    "        for status_file in sorted(output_path.rglob(\"case_status.json\")):\n",
    "            case_dir = status_file.parent\n",
    "            try:\n",
    "                with open(status_file) as fh:\n",
    "                    status = json.load(fh)\n",
    "            except (json.JSONDecodeError, OSError) as exc:\n",
    "                print(f\"⚠  Could not read {status_file}: {exc}\")\n",
    "                continue\n",
    "\n",
    "            records.append({\n",
    "                \"case_name\"       : case_dir.name,\n",
    "                \"case_path\"       : str(case_dir),\n",
    "                \"pipeline_status\" : derive_pipeline_status(status),\n",
    "                \"mesh_status\"     : status.get(\"mesh_status\"),\n",
    "                \"mesh_ok\"         : status.get(\"mesh_ok\"),\n",
    "                \"copied_to_hpc\"   : status.get(\"copied_to_hpc\"),\n",
    "                \"submitted\"       : status.get(\"submitted\"),\n",
    "                \"job_id\"          : status.get(\"job_id\"),\n",
    "                \"job_status\"      : status.get(\"job_status\"),\n",
    "                \"last_checked\"    : status.get(\"last_checked\"),\n",
    "            })\n",
    "\n",
    "    columns = [\n",
    "        \"case_name\", \"case_path\", \"pipeline_status\",\n",
    "        \"mesh_status\", \"mesh_ok\", \"copied_to_hpc\",\n",
    "        \"submitted\", \"job_id\", \"job_status\", \"last_checked\",\n",
    "    ]\n",
    "    df = pd.DataFrame(records, columns=columns) if records else pd.DataFrame(columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ── Scan ──────────────────────────────────────────────────────────────────────\n",
    "df_cases = scan_cases(CASES_OUTPUT_DIR)\n",
    "print(f\"Scanned {len(df_cases)} case(s) at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Dashboard\n",
    "\n",
    "Total case count, per-status breakdown, and focused tables for the statuses that need attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── High-level counts ────────────────────────────────────────────────────────\n",
    "STATUS_ORDER = [\"ready_to_mesh\", \"meshing\", \"meshed\", \"running\", \"complete\", \"failed\", \"unknown\"]\n",
    "\n",
    "total = len(df_cases)\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  CFD PIPELINE STATUS SUMMARY\")\n",
    "print(f\"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Total cases : {total}\")\n",
    "print()\n",
    "\n",
    "if total > 0:\n",
    "    counts = df_cases[\"pipeline_status\"].value_counts()\n",
    "    for status in STATUS_ORDER:\n",
    "        n = counts.get(status, 0)\n",
    "        if n > 0:\n",
    "            icon = {\"ready_to_mesh\": \"○\", \"meshing\": \"⟳\", \"meshed\": \"✓\",\n",
    "                    \"running\": \"▶\", \"complete\": \"★\", \"failed\": \"✗\"}.get(status, \"?\")\n",
    "            print(f\"  {icon}  {status:<16} : {n}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Per-status counts as a styled DataFrame ───────────────────────────────────\n",
    "if total > 0:\n",
    "    count_df = (\n",
    "        df_cases[\"pipeline_status\"]\n",
    "        .value_counts()\n",
    "        .reindex(STATUS_ORDER)\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "        .reset_index()\n",
    "    )\n",
    "    count_df.columns = [\"pipeline_status\", \"count\"]\n",
    "    count_df = count_df[count_df[\"count\"] > 0]\n",
    "    display(count_df.style.hide(axis=\"index\").set_caption(\"Cases per pipeline status\"))\n",
    "else:\n",
    "    print(\"No cases found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Ready-to-mesh cases ──────────────────────────────────────────────────────\n",
    "df_ready = df_cases[df_cases[\"pipeline_status\"] == \"ready_to_mesh\"][[\"case_name\", \"mesh_status\"]]\n",
    "print(f\"Ready to mesh: {len(df_ready)} case(s)\")\n",
    "if not df_ready.empty:\n",
    "    display(df_ready.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Running cases ────────────────────────────────────────────────────────────\n",
    "df_running = df_cases[df_cases[\"pipeline_status\"] == \"running\"][\n",
    "    [\"case_name\", \"job_id\", \"job_status\", \"last_checked\"]\n",
    "]\n",
    "print(f\"Running on HPC: {len(df_running)} case(s)\")\n",
    "if not df_running.empty:\n",
    "    display(df_running.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Failed cases ─────────────────────────────────────────────────────────────\n",
    "df_failed = df_cases[df_cases[\"pipeline_status\"] == \"failed\"][\n",
    "    [\"case_name\", \"mesh_status\", \"job_id\", \"job_status\"]\n",
    "]\n",
    "print(f\"Failed cases: {len(df_failed)}\")\n",
    "if not df_failed.empty:\n",
    "    display(df_failed.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Full case table ───────────────────────────────────────────────────────────\n",
    "print(\"Full case table:\")\n",
    "if total > 0:\n",
    "    display(\n",
    "        df_cases[\n",
    "            [\"case_name\", \"pipeline_status\", \"mesh_status\",\n",
    "             \"submitted\", \"job_id\", \"job_status\", \"last_checked\"]\n",
    "        ].reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    print(\"No cases found. Run generateInputs.py and the taskManager case generator first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Controlled Batch Runner\n",
    "\n",
    "Selects up to `BATCH_SIZE` cases whose `pipeline_status` is `ready_to_mesh` and triggers  \n",
    "`generator.mesh_cases_parallel()`.  \n",
    "\n",
    "**Resume-safe:** cases already meshed (or failed) are never re-selected — all filtering is based on `case_status.json`.  \n",
    "Re-run *Cell 3* after the batch completes to refresh the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generator is None:\n",
    "    print(\"✗ taskManager not available — cannot run batch.\")\n",
    "else:\n",
    "    # Derive ready-to-mesh cases directly from the JSON status files\n",
    "    # (not from the in-memory DataFrame, so this cell is safe to run\n",
    "    #  even if the DataFrame is stale)\n",
    "    all_ready = generator.list_cases_by_status(mesh_status=\"NOT_RUN\")\n",
    "    batch = all_ready[:BATCH_SIZE]\n",
    "\n",
    "    print(f\"Ready-to-mesh cases found : {len(all_ready)}\")\n",
    "    print(f\"Batch size configured     : {BATCH_SIZE}\")\n",
    "    print(f\"Cases selected for batch  : {len(batch)}\")\n",
    "\n",
    "    if not batch:\n",
    "        print(\"\\nNothing to do — no cases with mesh_status=NOT_RUN.\")\n",
    "        print(\"Possible reasons:\")\n",
    "        print(\"  • All cases have already been meshed (or are running/complete).\")\n",
    "        print(\"  • generateInputs.py has not been run yet.\")\n",
    "        print(\"  • CASES_OUTPUT_DIR is set incorrectly.\")\n",
    "    else:\n",
    "        print(\"\\nCases to be meshed in this batch:\")\n",
    "        for case_path in batch:\n",
    "            print(f\"  • {Path(case_path).name}\")\n",
    "\n",
    "        print(f\"\\nStarting parallel meshing with {N_PARALLEL_WORKERS} worker(s)…\")\n",
    "        results = generator.mesh_cases_parallel(batch, n_workers=N_PARALLEL_WORKERS)\n",
    "\n",
    "        succeeded = sum(results)\n",
    "        failed    = len(results) - succeeded\n",
    "        print(f\"\\nBatch complete: {succeeded} succeeded, {failed} failed.\")\n",
    "        print(\"Re-run Cell 3 to refresh the dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Refresh HPC Job Statuses\n",
    "\n",
    "Polls the SLURM scheduler on Deucalion for every submitted case and updates the local `case_status.json` files.  \n",
    "Requires SSH access to the `deucalion` host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generator is None:\n",
    "    print(\"✗ taskManager not available.\")\n",
    "else:\n",
    "    submitted_cases = generator.list_cases_by_status(submitted=True)\n",
    "    print(f\"Submitted cases to check: {len(submitted_cases)}\")\n",
    "\n",
    "    if not submitted_cases:\n",
    "        print(\"No submitted jobs to refresh.\")\n",
    "    else:\n",
    "        for case_path in submitted_cases:\n",
    "            new_status = generator.update_job_status(case_path)\n",
    "            print(f\"  {Path(case_path).name}: {new_status}\")\n",
    "\n",
    "        print(\"\\nJob statuses updated. Re-run Cell 3 to refresh the dashboard.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
