{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFD Pipeline Dashboard\n",
    "\n",
    "A lightweight workflow dashboard and controlled batch runner for the CFD pipeline.\n",
    "\n",
    "- **Dataset preparation** is handled by `generateInputs.py` (this repo).\n",
    "- **Simulation execution** is handled by `taskManager/taskManager.py` (submodule), which creates cases, meshes them, submits them to SLURM on HPC, and maintains a `case_status.json` per case.\n",
    "\n",
    "This notebook does **not** reimplement any execution logic \u2014 it only orchestrates and monitors by reading `case_status.json` files and calling existing `taskManager` methods when needed.\n",
    "\n",
    "**Resume-safe:** Close and reopen at any time. All decisions are derived from the `case_status.json` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit these paths and settings before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# \u2500\u2500 Paths \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Root of the CFD-dataset repository (directory containing this notebook)\n",
    "REPO_ROOT = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Directory that contains the generated OpenFOAM case folders\n",
    "# (each sub-folder has a case_status.json produced by taskManager)\n",
    "CASES_OUTPUT_DIR = os.path.join(REPO_ROOT, \"openFoamCases\")\n",
    "\n",
    "# OpenFOAM case template folder (used by taskManager to create new cases)\n",
    "TEMPLATE_PATH = os.path.join(REPO_ROOT, \"taskManager\", \"template\")\n",
    "\n",
    "# Input data directory (downloads from generateInputs.py)\n",
    "INPUT_DATA_DIR = os.path.join(REPO_ROOT, \"Data\", \"downloads\")\n",
    "\n",
    "# Remote HPC path on Deucalion (used by taskManager for rsync/sbatch)\n",
    "DEUCALION_PATH = \"/projects/EEHPC-BEN-2026B02-011/cfd_data\"\n",
    "\n",
    "# \u2500\u2500 Batch settings \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Number of cases to mesh per batch run (Cell 5)\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# Number of parallel workers for meshing\n",
    "N_PARALLEL_WORKERS = 4\n",
    "\n",
    "# \u2500\u2500 taskManager import \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Add the taskManager submodule directory to sys.path so we can import it\n",
    "TASK_MANAGER_DIR = os.path.join(REPO_ROOT, \"taskManager\")\n",
    "if TASK_MANAGER_DIR not in sys.path:\n",
    "    sys.path.insert(0, TASK_MANAGER_DIR)\n",
    "\n",
    "print(f\"REPO_ROOT          : {REPO_ROOT}\")\n",
    "print(f\"CASES_OUTPUT_DIR   : {CASES_OUTPUT_DIR}\")\n",
    "print(f\"TEMPLATE_PATH      : {TEMPLATE_PATH}\")\n",
    "print(f\"INPUT_DATA_DIR     : {INPUT_DATA_DIR}\")\n",
    "print(f\"BATCH_SIZE         : {BATCH_SIZE}\")\n",
    "print(f\"N_PARALLEL_WORKERS : {N_PARALLEL_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from taskManager import OpenFOAMCaseGenerator\n",
    "    print(\"\u2713 taskManager imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u2717 Could not import taskManager: {e}\")\n",
    "    print(\"  Make sure the taskManager submodule is initialised:\")\n",
    "    print(\"    git submodule update --init --recursive\")\n",
    "    OpenFOAMCaseGenerator = None\n",
    "\n",
    "# Initialise the case generator (used later for meshing / submission)\n",
    "if OpenFOAMCaseGenerator is not None:\n",
    "    generator = OpenFOAMCaseGenerator(\n",
    "        template_path=TEMPLATE_PATH,\n",
    "        input_dir=INPUT_DATA_DIR,\n",
    "        output_dir=CASES_OUTPUT_DIR,\n",
    "        deucalion_path=DEUCALION_PATH,\n",
    "    )\n",
    "    print(\"\u2713 OpenFOAMCaseGenerator initialised\")\n",
    "else:\n",
    "    generator = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Status Scanner\n",
    "\n",
    "Reads every `case_status.json` found under `CASES_OUTPUT_DIR` and assembles a pandas DataFrame.  \n",
    "Each row represents one case.  **Re-run this cell at any time to refresh the view.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_pipeline_status(status: dict) -> str:\n",
    "    \"\"\"\n",
    "    Map the raw fields in case_status.json to a single human-readable\n",
    "    pipeline status string:\n",
    "\n",
    "        ready_to_mesh  \u2013 mesh has not been run yet (mesh_status == NOT_RUN)\n",
    "        meshing        \u2013 meshing is currently in progress (mesh_status == IN_PROGRESS)\n",
    "        meshed         \u2013 mesh done, not yet submitted to HPC\n",
    "        running        \u2013 submitted to HPC, job is PENDING or RUNNING\n",
    "        complete       \u2013 HPC job completed successfully\n",
    "        failed         \u2013 meshing failed, or HPC job failed/cancelled/timed-out\n",
    "        unknown        \u2013 status file present but unrecognised combination\n",
    "    \"\"\"\n",
    "    mesh_status = (status.get(\"mesh_status\") or \"NOT_RUN\").upper()\n",
    "    job_status  = (status.get(\"job_status\")  or \"\").upper()\n",
    "    submitted   = status.get(\"submitted\", False)\n",
    "\n",
    "    if mesh_status in (\"FAILED\", \"ERROR\"):\n",
    "        return \"failed\"\n",
    "    if job_status in (\"FAILED\", \"CANCELLED\", \"TIMEOUT\"):\n",
    "        return \"failed\"\n",
    "    if job_status == \"COMPLETED\":\n",
    "        return \"complete\"\n",
    "    if submitted and job_status in (\"PENDING\", \"RUNNING\", \"\"):\n",
    "        return \"running\"\n",
    "    if mesh_status == \"DONE\":\n",
    "        return \"meshed\"\n",
    "    if mesh_status == \"IN_PROGRESS\":\n",
    "        return \"meshing\"\n",
    "    if mesh_status == \"NOT_RUN\":\n",
    "        return \"ready_to_mesh\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def scan_cases(output_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Walk *output_dir* and collect every case_status.json into a DataFrame.\n",
    "    Returns an empty DataFrame (with the expected columns) if no cases exist yet.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    records = []\n",
    "\n",
    "    if not output_path.exists():\n",
    "        print(f\"\u26a0  Cases output directory does not exist yet: {output_dir}\")\n",
    "        print(\"   Run generateInputs.py and the taskManager case generator first.\")\n",
    "    else:\n",
    "        for status_file in sorted(output_path.rglob(\"case_status.json\")):\n",
    "            case_dir = status_file.parent\n",
    "            try:\n",
    "                with open(status_file) as fh:\n",
    "                    status = json.load(fh)\n",
    "            except (json.JSONDecodeError, OSError) as exc:\n",
    "                print(f\"\u26a0  Could not read {status_file}: {exc}\")\n",
    "                continue\n",
    "\n",
    "            records.append({\n",
    "                \"case_name\"       : case_dir.name,\n",
    "                \"case_path\"       : str(case_dir),\n",
    "                \"pipeline_status\" : derive_pipeline_status(status),\n",
    "                \"mesh_status\"     : status.get(\"mesh_status\"),\n",
    "                \"mesh_ok\"         : status.get(\"mesh_ok\"),\n",
    "                \"copied_to_hpc\"   : status.get(\"copied_to_hpc\"),\n",
    "                \"submitted\"       : status.get(\"submitted\"),\n",
    "                \"job_id\"          : status.get(\"job_id\"),\n",
    "                \"job_status\"      : status.get(\"job_status\"),\n",
    "                \"last_checked\"    : status.get(\"last_checked\"),\n",
    "            })\n",
    "\n",
    "    columns = [\n",
    "        \"case_name\", \"case_path\", \"pipeline_status\",\n",
    "        \"mesh_status\", \"mesh_ok\", \"copied_to_hpc\",\n",
    "        \"submitted\", \"job_id\", \"job_status\", \"last_checked\",\n",
    "    ]\n",
    "    df = pd.DataFrame(records, columns=columns) if records else pd.DataFrame(columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "# \u2500\u2500 Scan \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "df_cases = scan_cases(CASES_OUTPUT_DIR)\n",
    "print(f\"Scanned {len(df_cases)} case(s) at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Dashboard\n",
    "\n",
    "Total case count, per-status breakdown, and focused tables for the statuses that need attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 High-level counts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "STATUS_ORDER = [\"ready_to_mesh\", \"meshing\", \"meshed\", \"running\", \"complete\", \"failed\", \"unknown\"]\n",
    "\n",
    "total = len(df_cases)\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  CFD PIPELINE STATUS SUMMARY\")\n",
    "print(f\"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Total cases : {total}\")\n",
    "print()\n",
    "\n",
    "if total > 0:\n",
    "    counts = df_cases[\"pipeline_status\"].value_counts()\n",
    "    for status in STATUS_ORDER:\n",
    "        n = counts.get(status, 0)\n",
    "        if n > 0:\n",
    "            icon = {\"ready_to_mesh\": \"\u25cb\", \"meshing\": \"\u27f3\", \"meshed\": \"\u2713\",\n",
    "                    \"running\": \"\u25b6\", \"complete\": \"\u2605\", \"failed\": \"\u2717\"}.get(status, \"?\")\n",
    "            print(f\"  {icon}  {status:<16} : {n}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Per-status counts as a styled DataFrame \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "if total > 0:\n",
    "    count_df = (\n",
    "        df_cases[\"pipeline_status\"]\n",
    "        .value_counts()\n",
    "        .reindex(STATUS_ORDER)\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "        .reset_index()\n",
    "    )\n",
    "    count_df.columns = [\"pipeline_status\", \"count\"]\n",
    "    count_df = count_df[count_df[\"count\"] > 0]\n",
    "    display(count_df.style.hide(axis=\"index\").set_caption(\"Cases per pipeline status\"))\n",
    "else:\n",
    "    print(\"No cases found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Ready-to-mesh cases \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "df_ready = df_cases[df_cases[\"pipeline_status\"] == \"ready_to_mesh\"][[\"case_name\", \"mesh_status\"]]\n",
    "print(f\"Ready to mesh: {len(df_ready)} case(s)\")\n",
    "if not df_ready.empty:\n",
    "    display(df_ready.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Running cases \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "df_running = df_cases[df_cases[\"pipeline_status\"] == \"running\"][\n",
    "    [\"case_name\", \"job_id\", \"job_status\", \"last_checked\"]\n",
    "]\n",
    "print(f\"Running on HPC: {len(df_running)} case(s)\")\n",
    "if not df_running.empty:\n",
    "    display(df_running.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Failed cases \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "df_failed = df_cases[df_cases[\"pipeline_status\"] == \"failed\"][\n",
    "    [\"case_name\", \"mesh_status\", \"job_id\", \"job_status\"]\n",
    "]\n",
    "print(f\"Failed cases: {len(df_failed)}\")\n",
    "if not df_failed.empty:\n",
    "    display(df_failed.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Full case table \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(\"Full case table:\")\n",
    "if total > 0:\n",
    "    display(\n",
    "        df_cases[\n",
    "            [\"case_name\", \"pipeline_status\", \"mesh_status\",\n",
    "             \"submitted\", \"job_id\", \"job_status\", \"last_checked\"]\n",
    "        ].reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    print(\"No cases found. Run generateInputs.py and the taskManager case generator first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate OpenFOAM Cases from Ready Inputs\n",
    "\n",
    "Discovers available inputs (terrain folders containing `pipeline_metadata.json`)\n",
    "inside `INPUT_DATA_DIR` and generates up to `BATCH_SIZE` new OpenFOAM case directories\n",
    "inside `CASES_OUTPUT_DIR`.\n",
    "\n",
    "An input is considered **already generated** when a matching `case_status.json` exists\n",
    "in the corresponding case folder (`case_{terrain_index}_{rotation:03d}deg/`).  \n",
    "Such inputs are silently skipped, making this step **resume-safe**.\n",
    "\n",
    "After this cell completes, re-run *Cell 3* to refresh the dashboard before proceeding\n",
    "to meshing (Section 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generator is None:\n",
    "    print(\"\\u2717 taskManager not available \\u2014 cannot generate cases.\")\n",
    "else:\n",
    "    # \u2500\u2500 Discover all available inputs from INPUT_DATA_DIR \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    all_inputs = generator.find_cases()\n",
    "    print(f\"Available inputs in INPUT_DATA_DIR : {len(all_inputs)}\")\n",
    "\n",
    "    # \u2500\u2500 Filter to inputs that do not yet have a generated case \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # A case is considered already generated if its output folder contains\n",
    "    # a case_status.json (written by initialize_case_status).\n",
    "    def _case_name(info):\n",
    "        return f\"case_{info['terrain_index']}_{int(info['rotation_degree']):03d}deg\"\n",
    "\n",
    "    pending_inputs = [\n",
    "        info for info in all_inputs\n",
    "        if not (Path(CASES_OUTPUT_DIR) / _case_name(info) / \"case_status.json\").exists()\n",
    "    ]\n",
    "    print(f\"Inputs not yet converted to cases  : {len(pending_inputs)}\")\n",
    "    print(f\"BATCH_SIZE configured              : {BATCH_SIZE}\")\n",
    "\n",
    "    batch_inputs = pending_inputs[:BATCH_SIZE]\n",
    "    print(f\"Cases to generate in this batch    : {len(batch_inputs)}\")\n",
    "\n",
    "    if not batch_inputs:\n",
    "        print(\"\\nNothing to do \\u2014 all available inputs have already been converted to cases.\")\n",
    "        print(\"Possible reasons:\")\n",
    "        print(\"  \\u2022 generateInputs.py has not been run yet (INPUT_DATA_DIR is empty).\")\n",
    "        print(\"  \\u2022 All discovered inputs already have a corresponding case folder.\")\n",
    "        print(\"  \\u2022 INPUT_DATA_DIR is set incorrectly.\")\n",
    "    else:\n",
    "        print(\"\\nInputs selected for case generation:\")\n",
    "        for info in batch_inputs:\n",
    "            print(f\"  \\u2022 terrain_{info['terrain_index']}  rotation={info['rotation_degree']}\\u00b0\"\n",
    "                  f\"  -> {_case_name(info)}\")\n",
    "\n",
    "        print(\"\\nGenerating cases\\u2026\")\n",
    "        generated = []\n",
    "        failed_gen = []\n",
    "        for info in batch_inputs:\n",
    "            try:\n",
    "                output_path = generator.setup_case(info)\n",
    "                print(f\"  \\u2713 {_case_name(info)} -> {output_path}\")\n",
    "                generated.append(output_path)\n",
    "            except Exception as exc:\n",
    "                print(f\"  \\u2717 {_case_name(info)}: {exc}\")\n",
    "                failed_gen.append(_case_name(info))\n",
    "\n",
    "        print(f\"\\nCase generation complete: {len(generated)} created, {len(failed_gen)} failed.\")\n",
    "        if generated:\n",
    "            print(\"Re-run Cell 3 to refresh the dashboard, then proceed to Section 6 for meshing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Controlled Batch Runner\n",
    "\n",
    "Selects up to `BATCH_SIZE` cases whose `pipeline_status` is `ready_to_mesh` and triggers  \n",
    "`generator.mesh_cases_parallel()`.  \n",
    "\n",
    "**Resume-safe:** cases already meshed (or failed) are never re-selected \u2014 all filtering is based on `case_status.json`.  \n",
    "Re-run *Cell 3* after the batch completes to refresh the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generator is None:\n",
    "    print(\"\u2717 taskManager not available \u2014 cannot run batch.\")\n",
    "else:\n",
    "    # Derive ready-to-mesh cases directly from the JSON status files\n",
    "    # (not from the in-memory DataFrame, so this cell is safe to run\n",
    "    #  even if the DataFrame is stale)\n",
    "    all_ready = generator.list_cases_by_status(mesh_status=\"NOT_RUN\")\n",
    "    batch = all_ready[:BATCH_SIZE]\n",
    "\n",
    "    print(f\"Ready-to-mesh cases found : {len(all_ready)}\")\n",
    "    print(f\"Batch size configured     : {BATCH_SIZE}\")\n",
    "    print(f\"Cases selected for batch  : {len(batch)}\")\n",
    "\n",
    "    if not batch:\n",
    "        print(\"\\nNothing to do \u2014 no cases with mesh_status=NOT_RUN.\")\n",
    "        print(\"Possible reasons:\")\n",
    "        print(\"  \u2022 All cases have already been meshed (or are running/complete).\")\n",
    "        print(\"  \u2022 generateInputs.py has not been run yet.\")\n",
    "        print(\"  \u2022 CASES_OUTPUT_DIR is set incorrectly.\")\n",
    "    else:\n",
    "        print(\"\\nCases to be meshed in this batch:\")\n",
    "        for case_path in batch:\n",
    "            print(f\"  \u2022 {Path(case_path).name}\")\n",
    "\n",
    "        print(f\"\\nStarting parallel meshing with {N_PARALLEL_WORKERS} worker(s)\u2026\")\n",
    "        results = generator.mesh_cases_parallel(batch, n_workers=N_PARALLEL_WORKERS)\n",
    "\n",
    "        succeeded = sum(results)\n",
    "        failed    = len(results) - succeeded\n",
    "        print(f\"\\nBatch complete: {succeeded} succeeded, {failed} failed.\")\n",
    "        print(\"Re-run Cell 3 to refresh the dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Optional) Refresh HPC Job Statuses\n",
    "\n",
    "Polls the SLURM scheduler on Deucalion for every submitted case and updates the local `case_status.json` files.  \n",
    "Requires SSH access to the `deucalion` host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generator is None:\n",
    "    print(\"\u2717 taskManager not available.\")\n",
    "else:\n",
    "    submitted_cases = generator.list_cases_by_status(submitted=True)\n",
    "    print(f\"Submitted cases to check: {len(submitted_cases)}\")\n",
    "\n",
    "    if not submitted_cases:\n",
    "        print(\"No submitted jobs to refresh.\")\n",
    "    else:\n",
    "        for case_path in submitted_cases:\n",
    "            new_status = generator.update_job_status(case_path)\n",
    "            print(f\"  {Path(case_path).name}: {new_status}\")\n",
    "\n",
    "        print(\"\\nJob statuses updated. Re-run Cell 3 to refresh the dashboard.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}